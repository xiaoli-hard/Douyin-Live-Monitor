import json
import os
import time
import datetime
import json
import logging
from typing import Optional
from openai import OpenAI
from src.ai_analysis.script_matching_analyzer import ScriptMatchingAnalyzer

# æ·»åŠ è¿™éƒ¨åˆ†ä»£ç 
def clean_emojis_for_storage(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ä¸­çš„ emoji å­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡å’Œæ­£å¸¸æ ‡ç‚¹"""
    if not text:
        return text
    import re
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„emojièŒƒå›´ï¼Œé¿å…è¯¯åˆ ä¸­æ–‡å­—ç¬¦
    emoji_pattern = re.compile(
        r'['
        r'\U0001F600-\U0001F64F'   # è¡¨æƒ…ç¬¦å·
        r'\U0001F300-\U0001F5FF'   # ç¬¦å·å’Œå›¾æ ‡
        r'\U0001F680-\U0001F6FF'   # è¿è¾“å’Œåœ°å›¾ç¬¦å·
        r'\U0001F1E0-\U0001F1FF'   # å›½æ——
        r'\U0001F900-\U0001F9FF'   # è¡¥å……ç¬¦å·
        r'\U00002600-\U000026FF'   # æ‚é¡¹ç¬¦å·
        r'\U00002700-\U000027BF'   # è£…é¥°ç¬¦å·
        r']+',
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text)

# é…ç½®æ—¥å¿— - é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)  # æ”¹ä¸ºINFOçº§åˆ«å‡å°‘æ—¥å¿—é‡
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

class DataAnalyzer:
    def __init__(self, client, config, root_dir: str):
        """åˆå§‹åŒ–æ—¶æ¥æ”¶é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"""
        self.client = client
        self.config = config
        self.root_dir = root_dir
        
        # --- æ‰€æœ‰è·¯å¾„éƒ½åŸºäº root_dir æ„å»º ---
        self.data_storage_path = os.path.join(self.root_dir, config['data_storage']['file_path'])
        self.hourly_log_path = os.path.join(self.root_dir, 'data', 'storage', 'hourly_data_log.json')
        
        # åˆå§‹åŒ–è¯æœ¯åŒ¹é…åˆ†æå™¨
        self.script_analyzer = ScriptMatchingAnalyzer(self.root_dir)
        self.strategy_library_path = os.path.join(self.root_dir, 'src', 'ai_analysis', 'strategy_library.json')
        self.speech_data_path = os.path.join(self.root_dir, config.get('speech_data', {}).get('file_path', 'text/latest_two_cleaned.json'))
        
        self.ensure_data_file_exists()

    def ensure_data_file_exists(self):
        """ç¡®ä¿æ•°æ®å­˜å‚¨æ–‡ä»¶å­˜åœ¨å¹¶åˆå§‹åŒ–"""
        try:
            if not os.path.exists(self.data_storage_path):
                # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
                os.makedirs(os.path.dirname(self.data_storage_path), exist_ok=True)
                with open(self.data_storage_path, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _load_strategy_library(self):
        """æ–°å¢æ–¹æ³•ï¼šåŠ è½½æˆ˜æœ¯ä¸è¯æœ¯åº“"""
        try:
            with open(self.strategy_library_path, 'r', encoding='utf-8') as f:
                return json.load(f).get('strategies', [])
        except FileNotFoundError:
            logger.error(f"ç­–ç•¥åº“æ–‡ä»¶æœªæ‰¾åˆ°: {self.strategy_library_path}")
            return []
        except Exception as e:
            logger.error(f"åŠ è½½ç­–ç•¥åº“å¤±è´¥: {str(e)}")
            return []

    def _get_diagnosis_from_ai(self, current_data, previous_data, speech_content, special_variables: Optional[str] = None):
        """ä¿®æ”¹æ–¹æ³•ï¼šæ”¹ä¸ºç›´æ¥ä»AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤ï¼Œå¹¶å¼ºåˆ¶å…¶å¿…é¡»è¿”å›å†…å®¹"""
        
        # ä¿®å¤ï¼šç¡®ä¿ä¼ é€’ç»™AIçš„æ˜¯çº¯å‡€çš„æŒ‡æ ‡æ•°æ®ï¼Œè€Œä¸æ˜¯åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´å¯¹è±¡
        current_pure_data = current_data.get('data', current_data)  # å¦‚æœæ˜¯å®Œæ•´å¯¹è±¡ï¼Œæå–dataå­—æ®µ
        previous_pure_data = previous_data.get('data', previous_data) if previous_data else {}
        
        # å¼ºåˆ¶è®°å½•ä¼ é€’ç»™è¯Šæ–­AIçš„åŸå§‹æ•°æ®
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯Šæ–­AIçš„å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}")
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯Šæ–­AIçš„å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}")
        
        # æ„å»ºå˜é‡ä¿¡æ¯éƒ¨åˆ†
        variables_prompt_part = ""
        if special_variables:
            variables_prompt_part = f"""
        **ä»Šæ—¥ç‰¹æ®Šå˜é‡**:
        {special_variables}
        ---
        """

        # æ„å»ºå®Œæ•´çš„Promptï¼Œè¦æ±‚AIåŒæ—¶æä¾›è¯Šæ–­å’Œå…·ä½“æˆ˜æœ¯æŒ‡ä»¤
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ¬§è±é›…æ´—å‘æ°´ç›´æ’­é”€å”®åˆ†æå¸ˆå’ŒæŠ¤å‘äº§å“è¥é”€ä¸“å®¶ã€‚è¯·å¯¹æ¯”ä»¥ä¸‹å½“å‰å°æ—¶å’Œä¸Šä¸€å°æ—¶çš„æ•°æ®ï¼Œä»¥åŠå½“å‰å°æ—¶çš„ä¸»æ’­è¯æœ¯ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯æ‰¾å‡ºæ ¸å¿ƒé—®é¢˜å¹¶æä¾›å…·ä½“çš„æ¬§è±é›…æ´—å‘æ°´è¥é”€æˆ˜æœ¯æŒ‡ä»¤æ¥æ”¹å–„é—®é¢˜ã€‚

        **é‡è¦è§„åˆ™ï¼šå¿…é¡»æä¾›è‡³å°‘ä¸€æ¡é’ˆå¯¹æ¬§è±é›…æ´—å‘æ°´äº§å“çš„æˆ˜æœ¯æŒ‡ä»¤ã€‚å¦‚æœæ•°æ®è¡¨ç°å¹³ç¨³æˆ–ä¼˜ç§€ï¼Œè¯·æä¾›ä¸€æ¡"ç»´æŒä¼˜åŠ¿"æˆ–"é”¦ä¸Šæ·»èŠ±"çš„é¼“åŠ±æ€§æŒ‡ä»¤ã€‚**
        
        **äº§å“èƒŒæ™¯ï¼šã€æ»‹å…»ä¿®å¤å‘è´¨ã€‘æ¬§è±é›…æ´—å‘æ°´æŠ¤å‘æŸ”é¡ºæ´—å‘éœ²æ¶¦å…»ç§€å‘å‘è´¨æ´—å‘ä¹³**
        
        {variables_prompt_part}
        å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}
        å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}
        è¯æœ¯å†…å®¹: {speech_content}
        
        é¦–å…ˆè¯Šæ–­é—®é¢˜ï¼Œæ‰¾å‡ºä»¥ä¸‹æ¬§è±é›…æ´—å‘æ°´ç›´æ’­å¸¸è§é—®é¢˜ä¸­å­˜åœ¨çš„1-3ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¯·è¯Šæ–­ä¸º"æ•°æ®è¡¨ç°å¹³ç¨³"ã€‚
        - äº§å“åŠŸæ•ˆè¯´æ˜ä¸å¤Ÿä¸“ä¸š/ç¼ºä¹æŠ¤å‘çŸ¥è¯†åˆ†äº«
        - å‘è´¨é—®é¢˜é’ˆå¯¹æ€§ä¸å¼º/å®¢ç¾¤å®šä½æ¨¡ç³Š
        - äº§å“ä½“éªŒæ„Ÿä¸è¶³/ç¼ºä¹ä½¿ç”¨æ•ˆæœå±•ç¤º
        - å“ç‰Œä¸“ä¸šåº¦ä½“ç°ä¸å¤Ÿ/ä¿¡ä»»æ„Ÿå»ºç«‹ä¸è¶³
        - ä»·æ ¼æ•æ„Ÿåº¦é«˜/ä»·å€¼å¡‘é€ ä¸å……åˆ†
        - äº’åŠ¨å¼•å¯¼ç¼ºä¹é’ˆå¯¹æ€§/å‘è´¨æµ‹è¯•ç¯èŠ‚ç¼ºå¤±
        - æ•°æ®è¡¨ç°å¹³ç¨³
        
        ç„¶åï¼Œå¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸€ä¸ªå…·ä½“çš„æ¬§è±é›…æ´—å‘æ°´è¥é”€æˆ˜æœ¯æŒ‡ä»¤ï¼ŒåŒ…æ‹¬:
        1. æˆ˜æœ¯åç§°ï¼šç®€çŸ­æœ‰åŠ›çš„æ ‡é¢˜ï¼ˆå¦‚ï¼šä¸“ä¸šæŠ¤å‘çŸ¥è¯†åˆ†äº«ã€å‘è´¨æµ‹è¯•äº’åŠ¨ã€äº§å“ä½“éªŒå±•ç¤ºç­‰ï¼‰
        2. ç›®æ ‡ï¼šè¿™ä¸ªæˆ˜æœ¯æƒ³è¦è¾¾æˆçš„æ•ˆæœï¼ˆæå‡å“ç‰Œä¸“ä¸šåº¦ã€å¢å¼ºäº§å“ä¿¡ä»»æ„Ÿã€ç²¾å‡†å®¢ç¾¤å®šä½ç­‰ï¼‰
        3. å…·ä½“æŒ‡ä»¤ï¼šè¯¦ç»†çš„æ‰§è¡Œæ–¹æ³•ï¼ŒåŒ…æ‹¬æ¬§è±é›…æ´—å‘æ°´ç›¸å…³çš„è¯æœ¯ç¤ºä¾‹ï¼ˆå¦‚ï¼š"è¿™æ¬¾æ¬§è±é›…æ´—å‘æ°´å«æœ‰æ»‹å…»ä¿®å¤æˆåˆ†..."ã€"é’ˆå¯¹æ‚¨çš„å‘è´¨é—®é¢˜ï¼Œæˆ‘æ¨è..."ç­‰ï¼‰
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæˆ–æ–‡æœ¬:
        {{
          "diagnoses": ["è¯Šæ–­å‡ºçš„é—®é¢˜1"],
          "strategies": [
            {{
              "id": "ai-gen-{int(time.time())}",
              "name": "æˆ˜æœ¯åç§°1",
              "goal": "æˆ˜æœ¯ç›®æ ‡1",
              "instruction": "è¯¦ç»†æŒ‡ä»¤å†…å®¹1ï¼ŒåŒ…æ‹¬å…·ä½“è¯æœ¯ç¤ºä¾‹"
            }}
          ]
        }}
        """
        try:
            logger.info("æ­£åœ¨è°ƒç”¨è±†åŒ…AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤...")
            response = self.client.chat.completions.create(
                model=self.config['douban_api']['model_name'],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"} # å¼€å¯JSONæ¨¡å¼ä»¥ç¡®ä¿æ ¼å¼æ­£ç¡®
            )
            ai_response_content = response.choices[0].message.content
            logger.info(f"æˆåŠŸä»AIè·å–åˆ°å“åº”: {ai_response_content}")
            
            # ç›´æ¥è§£æAIå“åº”ï¼Œä¸è¿›è¡Œé¢å¤–çš„å­—ç¬¦ä¸²æ¸…ç†
            # å› ä¸ºè¿‡åº¦çš„æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†å¯èƒ½ä¼šç ´åJSONç»“æ„
            try:
                return json.loads(ai_response_content)
            except json.JSONDecodeError as json_error:
                logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æ¸…ç†ç‰¹æ®Šæ ‡è®°: {json_error}")
                # æ¸…ç†è±†åŒ…APIå¯èƒ½è¿”å›çš„ç‰¹æ®Šæ ‡è®°å’Œå¤šä½™å†…å®¹
                cleaned_content = ai_response_content.strip()
                
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if cleaned_content.startswith('```json'):
                    cleaned_content = cleaned_content[7:]
                if cleaned_content.endswith('```'):
                    cleaned_content = cleaned_content[:-3]
                
                # ç§»é™¤è±†åŒ…APIçš„ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ <[PLHD30_never_used_xxx]>ï¼‰
                import re
                cleaned_content = re.sub(r'<\[PLHD30_never_used_[^>]+\]>', '', cleaned_content)
                
                # ç§»é™¤å¤šä½™çš„JSONå¯¹è±¡å’Œæ³¨é‡Šæ–‡å­—
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                brace_count = 0
                json_start = -1
                json_end = -1
                
                for i, char in enumerate(cleaned_content):
                    if char == '{' and json_start == -1:
                        json_start = i
                        brace_count = 1
                    elif json_start != -1:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                json_end = i + 1
                                break
                
                if json_start >= 0 and json_end > json_start:
                    cleaned_content = cleaned_content[json_start:json_end]
                
                # é¢å¤–å¤„ç†ï¼šç§»é™¤å¯èƒ½çš„ä¸­æ–‡æ³¨é‡Šå’Œè¯´æ˜æ–‡å­—
                cleaned_content = re.sub(r'ï¼ˆæ³¨ï¼š[^ï¼‰]*ï¼‰', '', cleaned_content)
                cleaned_content = re.sub(r'\s*,\s*"diagnoses".*$', '', cleaned_content, flags=re.DOTALL)
                
                cleaned_content = cleaned_content.strip()
                logger.info(f"æ¸…ç†åçš„JSONå†…å®¹: {cleaned_content[:200]}...")
                return json.loads(cleaned_content)
        except Exception as e:
            logger.error(f"ä»AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤å¤±è´¥: {e}", exc_info=True)
            # åœ¨APIå¤±è´¥æ—¶è¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„é»˜è®¤ç»“æœ
            return {
                "diagnoses": ["AIåˆ†æå¼‚å¸¸"], 
                "strategies": [{
                    "id": "error-fallback",
                    "name": "AIåˆ†ææœåŠ¡å‡ºç°é—®é¢˜",
                    "goal": "æç¤ºç”¨æˆ·æ£€æŸ¥åå°æœåŠ¡",
                    "instruction": f"è°ƒç”¨AIè¿›è¡Œåˆ†ææ—¶å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚é”™è¯¯è¯¦æƒ…: {str(e)}"
                }]
            }

    # ç§»é™¤ä¸å†éœ€è¦çš„åŒ¹é…æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„æˆ˜æœ¯
    # def _match_strategies(self, diagnoses_keywords, strategy_library):
    #    """æ–°å¢æ–¹æ³•ï¼šç¬¬äºŒæ­¥ - åŒ¹é…ç­–ç•¥ï¼Œæ­¤æ­¥ä¸è°ƒç”¨AI"""
    #    matched_strategies = []
    #    if not diagnoses_keywords:
    #        return matched_strategies
    #        
    #    for keyword in diagnoses_keywords:
    #        for strategy in strategy_library:
    #            if keyword in strategy.get('triggers', []):
    #                # é¿å…é‡å¤æ·»åŠ åŒä¸€ä¸ªç­–ç•¥
    #                if strategy not in matched_strategies:
    #                    matched_strategies.append(strategy)
    #    return matched_strategies

    def load_speech_data(self):
        """åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®"""
        logger.info(f"å¼€å§‹åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®ä»: {self.speech_data_path}")
        try:
            if not os.path.exists(self.speech_data_path):
                logger.warning(f"ä¸»æ’­è¯æœ¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.speech_data_path}")
                return []

            with open(self.speech_data_path, 'r', encoding='utf-8') as f:
                speech_data = json.load(f)
            
            logger.info(f"æˆåŠŸä»JSONæ–‡ä»¶åŠ è½½æ•°æ®ã€‚")
            if not isinstance(speech_data, list):
                logger.warning("ä¸»æ’­è¯æœ¯æ•°æ®æ ¼å¼åº”ä¸ºæ•°ç»„ï¼Œå·²è½¬æ¢ä¸ºå•å…ƒç´ æ•°ç»„")
                speech_data = [speech_data]
            
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(speech_data)} æ¡è®°å½•ã€‚")
            return speech_data
        except json.JSONDecodeError as e:
            logger.error(f"åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®å¤±è´¥: JSONè§£æé”™è¯¯ - {str(e)}", exc_info=True)
            return []
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
            return []

    def find_matching_speech(self, date_str, time_range):
        """æ ¹æ®æ—¥æœŸå’Œæ—¶é—´æ®µæŸ¥æ‰¾åŒ¹é…çš„ä¸»æ’­è¯æœ¯"""
        speech_data = self.load_speech_data()
        for entry in speech_data:
            # æ ‡å‡†åŒ–æ—¥æœŸå’Œæ—¶é—´æ®µæ ¼å¼è¿›è¡ŒåŒ¹é…
            entry_date = entry.get('æ—¥æœŸ', '')
            # æ ‡å‡†åŒ–æ—¥æœŸå’Œæ—¶é—´æ®µæ ¼å¼è¿›è¡ŒåŒ¹é…
            entry_date = entry.get('æ—¥æœŸ', '')
            original_time = entry.get('å°æ—¶', '')
            # å¤„ç†ä¸åŒæ ¼å¼çš„æ—¶é—´æ®µè¡¨ç¤º
            if 'ç‚¹' in original_time:
                # å¤„ç†'10ç‚¹-11ç‚¹'æ ¼å¼
                start_hour = original_time.split('-')[0].replace('ç‚¹', '').strip()
                entry_time = f"{start_hour}:00-{int(start_hour)+1}:00"
            else:
                # ç›´æ¥ä½¿ç”¨ç°æœ‰æ ¼å¼å¦‚'10:00-11:00'
                entry_time = original_time
            if entry_date == date_str and entry_time == time_range:
                return entry.get('text', '')
        logger.info(f"æœªæ‰¾åˆ°åŒ¹é…çš„ä¸»æ’­è¯æœ¯æ•°æ®: {date_str} {time_range}")
        return ""
    
    def load_data_from_csv(self):
        """ä» new_format_data.csv æ–‡ä»¶ä¸­è¯»å–æœ€åä¸¤è¡Œæ•°æ®ï¼ˆä¿®å¤ï¼šç›´æ¥ä»æ–‡ä»¶è¯»å–çœŸæ­£çš„æœ€åä¸¤è¡Œï¼‰"""
        try:
            csv_path = os.path.join(self.root_dir, 'data', 'baseline_data', 'æ¬§è±é›…æ•°æ®ç™»è®° - è‡ªåŠ¨åŒ–æ•°æ® (4).csv')
            if not os.path.exists(csv_path):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
                return None, None
            
            # ä¿®å¤ï¼šç›´æ¥ä»æ–‡ä»¶è¯»å–æœ€åä¸¤è¡Œï¼Œé¿å…pandasè·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
            import pandas as pd
            
            # é¦–å…ˆè¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œæ¥è·å–çœŸæ­£çš„æœ€åä¸¤è¡Œ
            with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            if len(lines) < 3:  # è‡³å°‘éœ€è¦å¤´éƒ¨+2è¡Œæ•°æ®
                logger.warning("CSVæ–‡ä»¶è¡Œæ•°ä¸è¶³")
                return None, None
            
            # è·å–å¤´éƒ¨å’Œæœ€åä¸¤è¡Œ
            header_line = lines[0].strip()
            last_line = lines[-1].strip()
            second_last_line = lines[-2].strip()
            
            logger.info(f"CSVæ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
            logger.info(f"çœŸæ­£çš„æœ€åä¸€è¡Œ(ç¬¬{len(lines)}è¡Œ): {last_line[:100]}...")
            logger.info(f"çœŸæ­£çš„å€’æ•°ç¬¬äºŒè¡Œ(ç¬¬{len(lines)-1}è¡Œ): {second_last_line[:100]}...")
            
            # è§£æå¤´éƒ¨è·å–åˆ—å
            headers = [h.strip() for h in header_line.split(',')]
            logger.info(f"CSVå¤´éƒ¨åˆ—æ•°: {len(headers)}")
            
            # è§£ææœ€åä¸¤è¡Œæ•°æ®
            def parse_csv_line(line, headers):
                """è§£æCSVè¡Œï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜"""
                values = [v.strip() for v in line.split(',')]
                # å¦‚æœå­—æ®µæ•°ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–å¡«å……
                if len(values) > len(headers):
                    logger.warning(f"è¡Œå­—æ®µæ•°({len(values)})è¶…è¿‡å¤´éƒ¨å­—æ®µæ•°({len(headers)})ï¼Œæˆªæ–­å¤šä½™å­—æ®µ")
                    values = values[:len(headers)]
                elif len(values) < len(headers):
                    logger.warning(f"è¡Œå­—æ®µæ•°({len(values)})å°‘äºå¤´éƒ¨å­—æ®µæ•°({len(headers)})ï¼Œå¡«å……ç©ºå€¼")
                    values.extend([''] * (len(headers) - len(values)))
                
                return dict(zip(headers, values))
            
            current_data = parse_csv_line(last_line, headers)
            previous_data = parse_csv_line(second_last_line, headers)
            
            # è®°å½•è¯»å–çš„æ•°æ®ç”¨äºè°ƒè¯•
            logger.info(f"è§£æåçš„å½“å‰æ•°æ®æ—¥æœŸ: {current_data.get('æ—¥æœŸ', 'N/A')} {current_data.get('å°æ—¶', 'N/A')}")
            logger.info(f"è§£æåçš„å†å²æ•°æ®æ—¥æœŸ: {previous_data.get('æ—¥æœŸ', 'N/A')} {previous_data.get('å°æ—¶', 'N/A')}")
            
            # è¯¦ç»†è®°å½•å…³é”®æŒ‡æ ‡çš„CSVåŸå§‹å€¼
            key_indicators = ['æ¶ˆè€—', 'æ•´ä½“GMV', 'æ•´ä½“ROI']
            for indicator in key_indicators:
                if indicator in current_data:
                    logger.info(f"CSVå½“å‰æ•°æ®(çœŸæ­£ç¬¬{len(lines)}è¡Œ) {indicator}: {current_data[indicator]}")
                if indicator in previous_data:
                    logger.info(f"CSVå†å²æ•°æ®(çœŸæ­£ç¬¬{len(lines)-1}è¡Œ) {indicator}: {previous_data[indicator]}")
            
            return current_data, previous_data
                
        except Exception as e:
            logger.error(f"ä»CSVæ–‡ä»¶è¯»å–æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None, None
    
    def load_speech_from_json(self, target_date, target_hour):
        """ç›´æ¥ä»è½¬å½•JSONæ–‡ä»¶ä¸­è¯»å–è¯æœ¯å†…å®¹"""
        try:
            # å°†æ—¥æœŸæ ¼å¼è½¬æ¢ä¸ºæ–‡ä»¶åæ ¼å¼ (2025-08-28 -> 2025-08-28)
            # å°†å°æ—¶æ ¼å¼è½¬æ¢ä¸ºæ–‡ä»¶åæ ¼å¼ (22:00-23:00 -> 22)
            hour_num = target_hour.split(':')[0] if ':' in target_hour else target_hour.split('-')[0].replace('ç‚¹', '').strip()
            
            # æ„å»ºJSONæ–‡ä»¶è·¯å¾„
            json_filename = f"transcripts_JSON_å®æ—¶_{target_date}_{hour_num.zfill(2)}.json"
            json_path = os.path.join(self.root_dir, 'text', json_filename)
            
            logger.info(f"æ­£åœ¨æŸ¥æ‰¾è¯æœ¯æ–‡ä»¶: {json_path}")
            
            if not os.path.exists(json_path):
                logger.warning(f"è¯æœ¯JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
                return ""
            
            # è¯»å–JSONæ–‡ä»¶å†…å®¹
            with open(json_path, 'r', encoding='utf-8') as f:
                transcript_data = json.load(f)
            
            if not isinstance(transcript_data, list):
                logger.warning(f"è¯æœ¯æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {json_path}")
                return ""
            
            # åˆå¹¶æ‰€æœ‰è¯æœ¯æ–‡æœ¬
            speech_texts = []
            for entry in transcript_data:
                text = entry.get('text', '')
                if text and text.strip():
                    speech_texts.append(text.strip())
            
            combined_speech = ' '.join(speech_texts)
            logger.info(f"æˆåŠŸè¯»å–è¯æœ¯å†…å®¹ï¼Œæ€»é•¿åº¦: {len(combined_speech)} å­—ç¬¦")
            
            return combined_speech
            
        except Exception as e:
            logger.error(f"ä»JSONæ–‡ä»¶è¯»å–è¯æœ¯å¤±è´¥: {e}", exc_info=True)
            return ""

    def get_previous_hour_data(self):
        """è·å–ä¸Šä¸€å°æ—¶çš„æ•°æ®"""
        try:
            # ä¿®å¤ï¼šä»æ–°çš„hourly_log_pathè¯»å–æ•°æ®ï¼Œè€Œä¸æ˜¯æ—§çš„data_storage_path
            if not os.path.exists(self.hourly_log_path):
                logger.warning(f"æ‰¾ä¸åˆ°å°æ—¶æ•°æ®æ—¥å¿—æ–‡ä»¶: {self.hourly_log_path}")
                return None
                
            with open(self.hourly_log_path, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
                
            if len(all_data) >= 2:
                return all_data[-2]  # è¿”å›å€’æ•°ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆä¸Šä¸€å°æ—¶ï¼‰
            elif len(all_data) == 1:
                logger.info("åªæœ‰ä¸€æ¡å†å²è®°å½•ï¼Œæ— æ³•è·å–ä¸Šä¸€å°æ—¶æ•°æ®")
                return None
            else:
                logger.warning("å†å²æ•°æ®æ—¥å¿—ä¸ºç©º")
                return None
        except Exception as e:
            logger.error(f"è·å–ä¸Šä¸€å°æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None

    def analyze_with_ai(self, current_data, previous_data, speech_content):
        """
        æ­¤æ–¹æ³•å°†è¢«åºŸå¼ƒï¼Œå…¶é€»è¾‘è¢«æ–°çš„ process_hourly_analysis æµç¨‹å–ä»£ã€‚
        ä¸ºä¿æŒå…¼å®¹ï¼Œæš‚æ—¶ä¿ç•™ä½†ä¸å†ä½¿ç”¨ã€‚
        """
        # æ„å»ºæç¤ºè¯
        # è·å–åˆ†æé˜ˆå€¼å¹¶è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        threshold_percent = self.config['analysis']['threshold'] * 100
        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        prompt = self.config['analysis']['prompt'].format(
            current_data=json.dumps(current_data['data'], ensure_ascii=False),
            previous_data=json.dumps(previous_data['data'], ensure_ascii=False),
            speech_content=speech_content,
            threshold=threshold_percent,
            current_time=current_time
        )
        
        # è°ƒç”¨è±†åŒ…AIè¿›è¡Œåˆ†æ
        response = self.client.chat.completions.create(
            model=self.config['douban_api']['model_name'],
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    def _generate_detailed_report_with_ai(self, current_data, previous_data, speech_content):
        """
        æ–°å¢æ–¹æ³•ï¼šä¸“é—¨ç”¨äºç”Ÿæˆæ—§ç‰ˆçš„ã€åŒ…å«è¯¦ç»†æ•°æ®è¡¨æ ¼å’Œåˆ†æçš„MarkdownæŠ¥å‘Šã€‚
        ä¿®å¤æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼šç¡®ä¿AIä½¿ç”¨çš„æ•°æ®ä¸CSVæ–‡ä»¶ä¸­çš„åŸå§‹æ•°æ®å®Œå…¨ä¸€è‡´ã€‚
        ä¿®å¤æŒ‡æ ‡æ˜ å°„é—®é¢˜ï¼šåŠ¨æ€è·å–é£ä¹¦æ•°æ®æºçš„çœŸå®æŒ‡æ ‡åç§°ï¼Œç¡®ä¿AIä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°ã€‚
        """
        threshold_percent = self.config['analysis']['threshold'] * 100
        current_time_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ä¿®å¤ï¼šç¡®ä¿ä¼ é€’ç»™AIçš„æ˜¯çº¯å‡€çš„æŒ‡æ ‡æ•°æ®ï¼Œè€Œä¸æ˜¯åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´å¯¹è±¡
        current_pure_data = current_data.get('data', current_data)  # å¦‚æœæ˜¯å®Œæ•´å¯¹è±¡ï¼Œæå–dataå­—æ®µ
        previous_pure_data = previous_data.get('data', previous_data) if previous_data else {}
        
        # å¼ºåˆ¶è®°å½•ä¼ é€’ç»™AIçš„åŸå§‹æ•°æ®
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯¦ç»†æŠ¥å‘ŠAIçš„å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}")
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯¦ç»†æŠ¥å‘ŠAIçš„å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}")
        
        # æ•°æ®ä¸€è‡´æ€§ä¿®å¤ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®ï¼Œç¡®ä¿ä¸CSVåŸå§‹æ•°æ®å®Œå…¨ä¸€è‡´
        def clean_data_for_ai(data_dict):
            """æ¸…ç†æ•°æ®å­—å…¸ï¼Œç§»é™¤NaNå€¼å’Œéæ•°å€¼æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§"""
            cleaned = {}
            for key, value in data_dict.items():
                # è·³è¿‡éæ•°å€¼åˆ—
                if key in ['æ—¥æœŸ', 'å°æ—¶', 'ä¸»æ’­', 'åœºæ§', 'åœºæ¬¡']:
                    cleaned[key] = str(value) if value is not None else ''
                else:
                    # å¤„ç†æ•°å€¼åˆ—
                    if value is None or str(value).lower() in ['nan', 'null', '']:
                        cleaned[key] = 0
                    else:
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œä¿æŒåŸå§‹ç²¾åº¦
                            if isinstance(value, (int, float)):
                                cleaned[key] = float(value)
                            else:
                                # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„æ•°å€¼
                                str_value = str(value).strip()
                                if str_value == '' or str_value.lower() in ['nan', 'null', 'none']:
                                    cleaned[key] = 0
                                else:
                                    cleaned[key] = float(str_value)
                        except (ValueError, TypeError):
                            logger.warning(f"æ— æ³•è½¬æ¢æ•°å€¼: {key}={value}, è®¾ç½®ä¸º0")
                            cleaned[key] = 0
            return cleaned
        
        # æ¸…ç†å½“å‰å’Œå†å²æ•°æ®
        current_clean_data = clean_data_for_ai(current_pure_data)
        previous_clean_data = clean_data_for_ai(previous_pure_data)
        
        # è®°å½•æ•°æ®æ¸…ç†æ—¥å¿—å’Œå…³é”®æŒ‡æ ‡å¯¹æ¯”
        logger.info(f"æ•°æ®æ¸…ç†å®Œæˆ - å½“å‰æ•°æ®æ¡ç›®æ•°: {len(current_clean_data)}, å†å²æ•°æ®æ¡ç›®æ•°: {len(previous_clean_data)}")
        
        # è¯¦ç»†è®°å½•å…³é”®æŒ‡æ ‡çš„åŸå§‹å€¼å’Œæ¸…ç†åçš„å€¼
        key_indicators = ['æ¶ˆè€—', 'æ•´ä½“GMV', 'æ•´ä½“ROI']
        for indicator in key_indicators:
            if indicator in current_pure_data and indicator in current_clean_data:
                logger.info(f"å½“å‰æ•°æ® {indicator}: åŸå§‹å€¼={current_pure_data[indicator]}, æ¸…ç†å={current_clean_data[indicator]}")
            if indicator in previous_pure_data and indicator in previous_clean_data:
                logger.info(f"å†å²æ•°æ® {indicator}: åŸå§‹å€¼={previous_pure_data[indicator]}, æ¸…ç†å={previous_clean_data[indicator]}")
        
        # ä¿®å¤æŒ‡æ ‡æ˜ å°„ï¼šåŠ¨æ€ç”ŸæˆæŒ‡æ ‡è¡¨æ ¼è¡Œï¼Œä½¿ç”¨é£ä¹¦æ•°æ®æºçš„çœŸå®æŒ‡æ ‡åç§°
        def generate_indicator_table_rows(data_dict):
            """æ ¹æ®å®é™…æ•°æ®åŠ¨æ€ç”ŸæˆæŒ‡æ ‡è¡¨æ ¼è¡Œ"""
            table_rows = []
            # æ’é™¤éæŒ‡æ ‡åˆ—
            excluded_columns = ['æ—¥æœŸ', 'å°æ—¶', 'ä¸»æ’­', 'åœºæ§', 'åœºæ¬¡']
            for key in data_dict.keys():
                if key not in excluded_columns:
                    table_rows.append(f"| {key} |        |          |            |      |      |")
            return "\n".join(table_rows)
        
        # ç”ŸæˆåŠ¨æ€æŒ‡æ ‡è¡¨æ ¼
        indicator_table_rows = generate_indicator_table_rows(current_clean_data)
        
        # ä½¿ç”¨ä¿®å¤åçš„Promptæ¨¡æ¿ï¼ŒåŠ¨æ€æ’å…¥çœŸå®æŒ‡æ ‡åç§°
        prompt = f"""åˆ†æä»¥ä¸‹ä¸¤ä¸ªå°æ—¶çš„ç›´æ’­æ•°æ®å¯¹æ¯”å’Œä¸»æ’­è¯æœ¯ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨å¼‚å¸¸æ³¢åŠ¨ï¼š

ã€å½“å‰å°æ—¶æ•°æ®ã€‘
{json.dumps(current_clean_data, ensure_ascii=False, indent=2)}

ã€ä¸Šä¸€å°æ—¶æ•°æ®ã€‘
{json.dumps(previous_clean_data, ensure_ascii=False, indent=2)}

ã€ä¸»æ’­è¯æœ¯æ‘˜è¦ã€‘
{speech_content}

è¯·æ‰§è¡Œä»¥ä¸‹æ·±åº¦åˆ†æï¼ˆä¸¥æ ¼æŒ‰æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿å†…å®¹è¯¦å®ï¼‰ï¼š
1. ã€å…¨é¢æŒ‡æ ‡åˆ†æã€‘å¯¹æ¯”æ‰€æœ‰æŒ‡æ ‡å·®å¼‚ï¼Œè®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰ï¼Œåˆ†æç»Ÿè®¡æ˜¾è‘—æ€§

2. ã€å¼‚å¸¸æ£€æµ‹ã€‘éµå¾ªä»¥ä¸‹æå…¶ä¸¥æ ¼çš„åˆ¤æ–­è§„åˆ™ï¼š
   - æ‰€æœ‰æŒ‡æ ‡ä¸Šæ¶¨ï¼Œæ— è®ºä¸Šæ¶¨å¤šå°‘ï¼Œå¿…é¡»æ ‡è®°ä¸ºğŸŸ¢æ­£å¸¸
   - æ‰€æœ‰æŒ‡æ ‡ä¸‹é™ä½†å¹…åº¦å°äº{threshold_percent}%ï¼Œå¿…é¡»æ ‡è®°ä¸ºğŸŸ¢æ­£å¸¸
   - ä»…å½“æŒ‡æ ‡ä¸‹é™å¹…åº¦è¶…è¿‡{threshold_percent}%æ—¶ï¼Œæ‰èƒ½æ ‡è®°ä¸ºğŸ”´å¼‚å¸¸
   - ç‰¹åˆ«æ³¨æ„ï¼šä¸Šæ¶¨çš„æŒ‡æ ‡ç»å¯¹ä¸èƒ½æ ‡è®°ä¸ºå¼‚å¸¸ï¼Œå³ä½¿ä¸Šæ¶¨å¹…åº¦å¾ˆå¤§

3. ã€æ¬§è±é›…æ´—å‘æ°´äº§å“åˆ†æã€‘
   - é‡ç‚¹å…³æ³¨ã€æ»‹å…»ä¿®å¤å‘è´¨ã€‘æ¬§è±é›…æ´—å‘æ°´æŠ¤å‘æŸ”é¡ºæ´—å‘éœ²æ¶¦å…»ç§€å‘å‘è´¨æ´—å‘ä¹³çš„æåŠæƒ…å†µ
   - åˆ†æäº§å“æ ¸å¿ƒå–ç‚¹æåŠï¼šæ»‹å…»ä¿®å¤ã€æŠ¤å‘æŸ”é¡ºã€æ¶¦å…»ç§€å‘ç­‰å…³é”®è¯é¢‘æ¬¡
   - è¯„ä¼°äº§å“åŠŸæ•ˆè¯æœ¯æ•ˆæœï¼šå‘è´¨æ”¹å–„ã€æŸ”é¡ºæ•ˆæœã€æ»‹å…»æˆåˆ†ç­‰æè¿°çš„è½¬åŒ–å½±å“
   - è¯†åˆ«ç›®æ ‡å®¢ç¾¤è¯æœ¯ï¼šé’ˆå¯¹å—æŸå‘è´¨ã€å¹²ç‡¥å‘è´¨ã€è¿½æ±‚æŸ”é¡ºæ•ˆæœç”¨æˆ·çš„è¯æœ¯ç­–ç•¥

4. ã€æ´—å‘æŠ¤å‘è¯æœ¯æ·±åº¦åˆ†æã€‘
   - æå–å…³é”®é”€å”®è¯æœ¯ï¼šäº§å“åŠŸæ•ˆä»‹ç»ã€ä½¿ç”¨æ–¹æ³•æŒ‡å¯¼ã€æ•ˆæœå¯¹æ¯”å±•ç¤º
   - åˆ†æä¸“ä¸šæŠ¤å‘æœ¯è¯­ä½¿ç”¨ï¼šæ°¨åŸºé…¸ã€è›‹ç™½è´¨ä¿®å¤ã€æ·±å±‚æ»‹å…»ç­‰ä¸“ä¸šè¯æ±‡æ•ˆæœ
   - è¯„ä¼°äº’åŠ¨å¼•å¯¼ç­–ç•¥ï¼šå‘è´¨æµ‹è¯•ã€ä½¿ç”¨ä½“éªŒåˆ†äº«ã€å‰åå¯¹æ¯”ç­‰äº’åŠ¨æ–¹å¼
   - å»ºç«‹è¯æœ¯ä¸æŒ‡æ ‡å…³è”æ€§ï¼šåŠŸæ•ˆå¼ºè°ƒä¸è½¬åŒ–ç‡ã€ä¸“ä¸šåº¦ä¸å®¢å•ä»·å…³ç³»

5. ã€æ ¹å› è¯Šæ–­ã€‘ç»“åˆæ•°æ®ä¸è¯æœ¯æä¾›3-5ä¸ªå¯èƒ½åŸå› ï¼Œæ¯ä¸ªåŸå› éœ€åŒ…å«ï¼š
   - å…·ä½“æ•°æ®è¯æ®ï¼ˆæŒ‡æ ‡å˜åŒ–å€¼ï¼‰
   - ç›¸å…³è¯æœ¯ç‰‡æ®µï¼ˆç›´æ¥å¼•ç”¨ï¼‰
   - å› æœå…³ç³»è§£é‡Š

6. ã€è¶‹åŠ¿é¢„æµ‹ã€‘åŸºäºå½“å‰æ•°æ®å’Œè¯æœ¯æ•ˆæœé¢„æµ‹ä¸‹ä¸€å°æ—¶å¯èƒ½è¶‹åŠ¿

7. ã€é¢„è­¦ä¿¡æ¯ã€‘å¦‚æœ‰å¼‚å¸¸ï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†çº§ï¼ˆP0-P2ï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨å¢å¼ºMarkdownæ ¼å¼ï¼Œç¡®ä¿è§†è§‰æ¸…æ™°ï¼‰ï¼š
## ğŸ“Š æŒ‡æ ‡å˜åŒ–åˆ†æ
**é‡è¦æç¤ºï¼šå¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æŒ‡æ ‡çš„å¯¹æ¯”ï¼Œä½¿ç”¨é£ä¹¦æ•°æ®æºä¸­çš„çœŸå®æŒ‡æ ‡åç§°ï¼Œä¸èƒ½çœç•¥ä»»ä½•æŒ‡æ ‡**
| æŒ‡æ ‡åç§° | å½“å‰å€¼ | ä¸Šå°æ—¶å€¼ | å˜åŒ–ç™¾åˆ†æ¯” | è¶‹åŠ¿ | çŠ¶æ€ |
|----------|--------|----------|------------|------|------|
{indicator_table_rows}
> **çŠ¶æ€è¯´æ˜**ï¼šğŸ”´ å¼‚å¸¸ï¼ˆä¸‹é™è¶…è¿‡{threshold_percent}%ï¼‰ | ğŸŸ¢ æ­£å¸¸ï¼ˆä¸Šæ¶¨æˆ–ä¸‹é™ä¸è¶³{threshold_percent}%ï¼‰

## ğŸ” æ¬§è±é›…æ´—å‘æ°´äº§å“åˆ†æ
| å…³é”®è¯ç±»å‹ | å…·ä½“å†…å®¹ | æåŠæ¬¡æ•° | è½¬åŒ–æ•ˆæœ |
|------------|----------|----------|----------|
| äº§å“å…¨ç§° | æ¬§è±é›…æ´—å‘æ°´/æŠ¤å‘æŸ”é¡ºæ´—å‘éœ² | [æ¬¡æ•°] | [è½¬åŒ–ç‡å˜åŒ–] |
| æ ¸å¿ƒåŠŸæ•ˆ | æ»‹å…»ä¿®å¤/æŠ¤å‘æŸ”é¡º/æ¶¦å…»ç§€å‘ | [æ¬¡æ•°] | [å®¢å•ä»·å½±å“] |
| ç›®æ ‡å‘è´¨ | å—æŸå‘è´¨/å¹²ç‡¥å‘è´¨/æ¯›èºå‘è´¨ | [æ¬¡æ•°] | [æˆäº¤äººæ•°å˜åŒ–] |
| ä¸“ä¸šæœ¯è¯­ | æ°¨åŸºé…¸/è›‹ç™½è´¨ä¿®å¤/æ·±å±‚æ»‹å…» | [æ¬¡æ•°] | [è§‚çœ‹æ—¶é•¿å½±å“] |

## âš ï¸ å¼‚å¸¸æŒ‡æ ‡é¢„è­¦
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„åµŒå¥—åˆ—è¡¨æ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨4ä¸ªç©ºæ ¼è¿›è¡Œç¼©è¿›åˆ›å»ºå­åˆ—è¡¨:
- **æŒ‡æ ‡åç§° (å˜åŒ–ç™¾åˆ†æ¯”)**:
    - **åŸå› åˆ†æ**: [AIåˆ†æçš„åŸå› ]
    - **æ•°æ®è¯æ®**: [å¼•ç”¨çš„å…·ä½“æ•°æ®]
    - **è¯æœ¯è¯æ®**: [å¼•ç”¨çš„ç›¸å…³è¯æœ¯]

## ğŸ’¡ æ¬§è±é›…æ´—å‘æ°´è¥é”€ä¼˜åŒ–å»ºè®®
1. **äº§å“å±•ç¤ºä¼˜åŒ–**: åŠ å¼ºå‘è´¨å¯¹æ¯”å±•ç¤ºï¼Œçªå‡ºæ»‹å…»ä¿®å¤æ•ˆæœçš„å¯è§†åŒ–å‘ˆç°
2. **è¯æœ¯ç­–ç•¥è°ƒæ•´**: å¢åŠ ä¸“ä¸šæŠ¤å‘çŸ¥è¯†åˆ†äº«ï¼Œæå‡å“ç‰Œä¸“ä¸šåº¦å’Œç”¨æˆ·ä¿¡ä»»æ„Ÿ
3. **äº’åŠ¨å¼•å¯¼å¼ºåŒ–**: è®¾è®¡å‘è´¨æµ‹è¯•ç¯èŠ‚ï¼Œè®©ç”¨æˆ·å‚ä¸äº§å“é€‚é…æ€§åˆ¤æ–­
4. **åŠŸæ•ˆå¼ºè°ƒé‡ç‚¹**: é‡ç‚¹çªå‡º"æ»‹å…»ä¿®å¤"ã€"æŠ¤å‘æŸ”é¡º"ç­‰æ ¸å¿ƒå–ç‚¹çš„å…·ä½“æ•ˆæœ
5. **å®¢ç¾¤ç²¾å‡†å®šä½**: é’ˆå¯¹ä¸åŒå‘è´¨é—®é¢˜ï¼ˆå¹²ç‡¥ã€å—æŸã€æ¯›èºï¼‰æä¾›ä¸ªæ€§åŒ–è§£å†³æ–¹æ¡ˆ

> **åˆ†æå‘¨æœŸ**ï¼š{current_time_str} | **æ•°æ®æ¥æº**ï¼šé£ä¹¦è¡¨æ ¼"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.config['douban_api']['model_name'],
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯¦ç»†AIåˆ†ææŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return f"# AIåˆ†æé”™è¯¯\n\nåœ¨ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"


    def process_hourly_analysis(self, special_variables: Optional[str] = None):
        """å¤„ç†å°æ—¶çº§åˆ†æï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œç»“æ„åŒ–ç»“æœè¿”å›"""
        try:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥special_variablesï¼Œè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
            if special_variables is None:
                special_variables = ""
            
            current_data, previous_data = self.load_data_from_csv()
            
            if not current_data:
                message = "æ— æ³•ä»CSVæ–‡ä»¶è¯»å–æ•°æ®"
                logger.error(message)
                return {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "diagnoses": ["æ•°æ®è¯»å–å¤±è´¥"],
                    "recommended_strategies": [],
                    "report_markdown": f"# {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')} ç›´æ’­å¤ç›˜AIæŒ‡ä»¤\n\n{message}"
                }
            
            # è·å–å½“å‰æ•°æ®çš„æ—¥æœŸå’Œå°æ—¶ä¿¡æ¯
            current_date = current_data.get('æ—¥æœŸ', '')
            current_hour = current_data.get('å°æ—¶', '')
            
            # ä»JSONæ–‡ä»¶åŒ¹é…å½“å‰å°æ—¶çš„è¯æœ¯å†…å®¹
            current_speech_content = self.load_speech_from_json(current_date, current_hour)
            
            # è·å–ä¸Šä¸€å°æ—¶çš„è¯æœ¯å†…å®¹ï¼ˆå¦‚æœæœ‰ä¸Šä¸€å°æ—¶æ•°æ®ï¼‰
            previous_speech_content = ""
            if previous_data:
                previous_date = previous_data.get('æ—¥æœŸ', '')
                previous_hour = previous_data.get('å°æ—¶', '')
                previous_speech_content = self.load_speech_from_json(previous_date, previous_hour)
            
            logger.info(f"å½“å‰æ•°æ®: {current_date} {current_hour}")
            logger.info(f"å½“å‰è¯æœ¯å†…å®¹é•¿åº¦: {len(current_speech_content)}")
            if previous_data:
                logger.info(f"ä¸Šä¸€å°æ—¶æ•°æ®: {previous_date} {previous_hour}")
                logger.info(f"ä¸Šä¸€å°æ—¶è¯æœ¯å†…å®¹é•¿åº¦: {len(previous_speech_content)}")

            if not previous_data:
                # å³ä½¿æ²¡æœ‰å†å²æ•°æ®ï¼Œä¹Ÿè¿”å›æ ‡å‡†æ ¼å¼
                message = "é¦–æ¬¡è¿è¡Œï¼Œæ— å†å²æ•°æ®å¯ä¾›å¯¹æ¯”åˆ†æ"
                logger.info(message)
                return {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "diagnoses": ["é¦–æ¬¡è¿è¡Œ"],
                    "recommended_strategies": [],
                    "report_markdown": f"# {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')} ç›´æ’­å¤ç›˜AIæŒ‡ä»¤\n\n{message}"
                }

            # æ„å»ºæ•°æ®ç»“æ„ç”¨äºAIåˆ†æ
            current_entry = {
                'timestamp': f"{current_date} {current_hour.split('-')[0] if '-' in current_hour else current_hour}",
                'data': current_data,
                'speech_content': current_speech_content
            }
            
            previous_entry = {
                'timestamp': f"{previous_date} {previous_hour.split('-')[0] if '-' in previous_hour else previous_hour}",
                'data': previous_data,
                'speech_content': previous_speech_content
            }

            # è¯æœ¯åŒ¹é…åˆ†æ
            script_analysis_result = None
            script_analysis_md = ""
            logger.info(f"æ£€æŸ¥è¯æœ¯å†…å®¹: {'æœ‰å†…å®¹' if current_speech_content else 'æ— å†…å®¹'}")
            if current_speech_content:
                try:
                    # æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼Œè®°å½•ä¼ å…¥è¯æœ¯åˆ†æå™¨çš„å†…å®¹
                    logger.info(f"å‡†å¤‡è¿›è¡Œè¯æœ¯åŒ¹é…åˆ†æï¼Œä¼ å…¥å†…å®¹é•¿åº¦: {len(current_speech_content)}")
                    logger.debug(f"ä¼ å…¥è¯æœ¯å†…å®¹ (å‰100å­—ç¬¦): {current_speech_content[:100]}")

                    script_analysis_result = self.script_analyzer.analyze_script_coverage(current_speech_content)
                    
                    # æ·»åŠ æ—¥å¿—ï¼Œè®°å½•è¦†ç›–ç‡åˆ†æç»“æœ
                    logger.info(f"è¯æœ¯è¦†ç›–ç‡åˆ†æå®Œæˆ: {json.dumps(script_analysis_result, ensure_ascii=False)}")

                    script_analysis_md = self.script_analyzer.generate_script_matching_report(current_speech_content, current_data)
                    
                    logger.info(f"è¯æœ¯åŒ¹é…åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼Œæ•´ä½“è¦†ç›–ç‡: {script_analysis_result['overall_coverage']*100:.1f}%")

                except Exception as e:
                    # ä½¿ç”¨ exc_info=True è®°å½•å®Œæ•´çš„å †æ ˆè·Ÿè¸ª
                    logger.error(f"è¯æœ¯åŒ¹é…åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
                    script_analysis_md = "\n\n## ğŸ¯ è¯æœ¯æ¨¡æ¿åŒ¹é…åˆ†æ\n\nâŒ è¯æœ¯åˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ (å†…éƒ¨é”™è¯¯)\n\n"
            else:
                script_analysis_md = "\n\n## ğŸ¯ è¯æœ¯æ¨¡æ¿åŒ¹é…åˆ†æ\n\nâš ï¸ æœ¬å°æ—¶æ— è¯æœ¯å†…å®¹è®°å½•\n\n"

            # å…³é”®æ”¹åŠ¨ï¼šé¦–å…ˆç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
            detailed_report_md = self._generate_detailed_report_with_ai(current_entry, previous_entry, current_speech_content)

            # æ·»åŠ åŠ¨æ€åŸºçº¿å¯¹æ¯”åˆ†æ
            from src.baseline.dynamic_baseline_engine import RealDataDynamicBaseline

            # åˆå§‹åŒ–åŸºçº¿å¼•æ“
            baseline_engine = RealDataDynamicBaseline(data_dir=os.path.join(self.root_dir, 'data'))
            baseline_data_path = os.path.join(self.root_dir, 'data', 'baseline_data', 'æ¬§è±é›…æ•°æ®ç™»è®° - è‡ªåŠ¨åŒ–æ•°æ® (4).csv')
            if not baseline_engine.is_initialized:
                baseline_engine.initialize_system(baseline_data_path)

            # å‡†å¤‡åŸºçº¿æŸ¥è¯¢æ•°æ®
            current_time = datetime.datetime.fromisoformat(current_entry['timestamp'])
            query_data = {
                "æ˜ŸæœŸå‡ ": current_time.weekday(),
                "å°æ—¶": current_time.hour,
                "ä¸»æ’­": current_entry.get('anchor', ''),
                **current_entry['data']
            }

            # è·å–åŸºçº¿åˆ†æç»“æœ
            baseline_result = baseline_engine.real_time_diagnosis(query_data)
            
            # è°ƒè¯•ï¼šè¾“å‡ºå®Œæ•´çš„åŸºçº¿ç»“æœç»“æ„
            logger.info(f"ğŸ” å®Œæ•´åŸºçº¿ç»“æœ: {json.dumps(baseline_result, ensure_ascii=False, indent=2)}")

            # æ ¼å¼åŒ–åŸºçº¿åˆ†æç»“æœä¸ºMarkdown
            baseline_md = "\n\n---\n\n## ğŸ“Š åŠ¨æ€åŸºçº¿å¯¹æ¯”åˆ†æ\n\n"
            if 'error' in baseline_result:
                baseline_md += f"**é”™è¯¯ä¿¡æ¯**: {baseline_result['error']}\n\n"
            else:
                baseline_md += f"**åˆ†ææ—¶æ®µ**: {baseline_result['æŸ¥è¯¢æ—¶æ®µ']}\n\n"
                baseline_md += "### æŒ‡æ ‡è¯„ä¼°ç»“æœ\n\n"
                baseline_md += "| æŒ‡æ ‡åç§° | è¯„ä¼°ç»“æœ | ç³»æ•° | åŸºçº¿å€¼ | è¯„ä¼°æ–¹æ³• |\n"
                baseline_md += "|----------|----------|------|--------|----------|\n"
                for indicator, result in baseline_result['è¯„ä¼°ç»“æœ'].items():
                    # ä¿®å¤åŸºçº¿å€¼æå–é€»è¾‘
                    baseline_value = 'N/A'
                    if 'åŸºçº¿å€¼' in result:
                        baseline_value = result['åŸºçº¿å€¼']
                    elif 'åŠ¨æ€è¯¦æƒ…' in result and 'åŸºçº¿å€¼' in result['åŠ¨æ€è¯¦æƒ…']:
                        baseline_value = result['åŠ¨æ€è¯¦æƒ…']['åŸºçº¿å€¼']
                    logger.info(f"ğŸ” è°ƒè¯•åŸºçº¿å€¼æå– - æŒ‡æ ‡: {indicator}, ç»“æœ: {result}, æå–çš„åŸºçº¿å€¼: {baseline_value}")
                    
                    baseline_md += f"| {indicator} | {result['è¯„ä¼°']} | {result['ç³»æ•°']} | {baseline_value} | {result['è¯„ä¼°æ–¹æ³•']} |\n"

            # å°†åŸºçº¿åˆ†æå’Œè¯æœ¯åˆ†ææ·»åŠ åˆ°æŠ¥å‘Š
            detailed_report_md += baseline_md
            detailed_report_md += script_analysis_md

            # 1. (è¯Šæ–­) è°ƒç”¨AIè·å–ç»“æ„åŒ–çš„è¯Šæ–­å…³é”®è¯å’Œæˆ˜æœ¯æŒ‡ä»¤
            diagnosis_result = self._get_diagnosis_from_ai(current_entry, previous_entry, current_speech_content, special_variables)
            diagnoses_keywords = diagnosis_result.get("diagnoses", [])
            matched_strategies = diagnosis_result.get("strategies", []) # ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„æˆ˜æœ¯
            
            # 2. (æ•´åˆ) å°†æ–°çš„AIæŒ‡ä»¤è¿½åŠ åˆ°è¯¦ç»†æŠ¥å‘Šæœ«å°¾
            final_report_md = detailed_report_md
            if matched_strategies:
                instructions_md_parts = [
                    "\n\n---\n\n",
                    "## ğŸ¤– AIæˆ˜æœ¯æŒ‡ä»¤\n\n",
                    f"**AIè¯Šæ–­å‡ºçš„æ ¸å¿ƒé—®é¢˜æ˜¯**: {', '.join(diagnoses_keywords)}\n\n",
                    "**[AIæŒ‡ä»¤]** ä¸»æ’­åŠåœºæ§è¯·æ³¨æ„ï¼Œè¯·ç«‹å³æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
                ]
                for i, strategy in enumerate(matched_strategies, 1):
                    # æ·»åŠ è°ƒè¯•æ—¥å¿—
                    logger.info(f"å¤„ç†ç­–ç•¥ {i}: {strategy}")
                    
                    # æ·»åŠ ä»¥ä¸‹å‡ è¡Œä»£ç ï¼Œæ¸…ç†ç­–ç•¥ä¸­çš„emoji
                    raw_name = strategy.get('name', '')
                    raw_goal = strategy.get('goal', '')
                    raw_instruction = strategy.get('instruction', '')
                    
                    logger.info(f"åŸå§‹æ•°æ® - name: '{raw_name}', goal: '{raw_goal}', instruction: '{raw_instruction[:100]}...'")
                    
                    clean_name = clean_emojis_for_storage(raw_name)
                    clean_goal = clean_emojis_for_storage(raw_goal)
                    clean_instruction = clean_emojis_for_storage(raw_instruction)
                    
                    logger.info(f"æ¸…ç†åæ•°æ® - name: '{clean_name}', goal: '{clean_goal}', instruction: '{clean_instruction[:100]}...'")
                    
                    instructions_md_parts.append(
                        f"\n**{i}. {clean_name} (ç›®æ ‡: {clean_goal})**\n"
                        f"   - **æŒ‡ä»¤è¯¦æƒ…**: {clean_instruction}\n"
                    )
                final_report_md += "".join(instructions_md_parts)

            return {
                "timestamp": datetime.datetime.now().isoformat(),
                "diagnoses": diagnoses_keywords,
                "recommended_strategy_ids": [s.get('id') for s in matched_strategies], # è¿”å›ç­–ç•¥ID
                "recommended_strategies": matched_strategies, # ä¿å­˜å®Œæ•´çš„æˆ˜æœ¯æŒ‡ä»¤
                "script_analysis": script_analysis_result,  # æ·»åŠ è¯æœ¯åˆ†æç»“æœ
                "report_markdown": final_report_md
            }
        
        except Exception as e:
            logger.error(f"å¤„ç†å°æ—¶çº§åˆ†ææ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            # ä¿®å¤ï¼šä½¿ç”¨clean_emojis_for_storageå‡½æ•°æ¸…ç†é”™è¯¯æ¶ˆæ¯ä¸­çš„emojiå­—ç¬¦
            error_message = clean_emojis_for_storage(str(e))
            return {
                "timestamp": datetime.datetime.now().isoformat(),
                "diagnoses": ["Error"],
                "recommended_strategies": [],
                "report_markdown": f"# åˆ†ææµç¨‹é”™è¯¯\n\nå¤„ç†æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {error_message}"
            }


# åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼Œimportè¯­å¥ä¹‹åæ·»åŠ è¾…åŠ©å‡½æ•°
import re

# def clean_emojis_for_storage(text):
#     """æ¸…ç†æ–‡æœ¬ä¸­çš„emojiå­—ç¬¦ï¼Œé¿å…åœ¨å­˜å‚¨å’Œå¤„ç†æ—¶å‡ºç°ç¼–ç é—®é¢˜"""
#     if not text:
#         return text
#     # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤emojiå­—ç¬¦
#     emoji_pattern = re.compile("[" 
#         u"\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
#         u"\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œå›¾æ ‡
#         u"\U0001F680-\U0001F6FF"  # è¿è¾“å’Œåœ°å›¾ç¬¦å·
#         u"\U0001F1E0-\U0001F1FF"  # å›½æ——
#         u"\U00002702-\U000027B0"  # å„ç§ç¬¦å·
#         u"\U000024C2-\U0001F251"  # å„ç§ç¬¦å·
#         ""]+", flags=re.UNICODE)
#     return emoji_pattern.sub(r'', text)



# ä»å®ä¾‹æ–¹æ³•æ”¹ä¸ºæ™®é€šå‡½æ•°ï¼Œç§»é™¤selfå‚æ•°
def save_analysis_result(analysis_output: dict, root_dir: str):
    """
    ä¿å­˜åˆ†æç»“æœä¸ºMarkdownæŠ¥å‘Šã€‚
    
    Args:
        analysis_output (dict): åŒ…å«æŠ¥å‘Šå†…å®¹çš„å­—å…¸ã€‚
        root_dir (str): é¡¹ç›®çš„æ ¹ç›®å½• (conclusion/) çš„ç»å¯¹è·¯å¾„ã€‚
    """
    report_content = analysis_output.get("report_markdown")
    if not report_content:
        logger.error("åˆ†æç»“æœä¸­ç¼ºå°‘'report_content'ï¼Œæ— æ³•ä¿å­˜æŠ¥å‘Šã€‚")
        return

    # --- ä½¿ç”¨ root_dir æ„å»ºå¥å£®çš„æŠ¥å‘Šä¿å­˜è·¯å¾„ ---
    reports_dir = os.path.join(root_dir, 'analysis_reports')
    os.makedirs(reports_dir, exist_ok=True)
    
    timestamp_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    file_name = f"{timestamp_str}_analysis_result.md"
    file_path = os.path.join(reports_dir, file_name)

    try:
        # ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        logger.info(f"åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜è‡³: {file_path}")
    except IOError as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        raise
    
    # æ­¤å¤–ï¼Œä¹Ÿå°†ç»“æ„åŒ–æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­
    results_path = os.path.join(root_dir, 'data', 'results', 'analysis_results.json')
    try:
        all_results = []
        if os.path.exists(results_path):
            with open(results_path, 'r', encoding='utf-8') as f:
                try:
                    all_results = json.load(f)
                    if not isinstance(all_results, list):
                        all_results = []
                except json.JSONDecodeError:
                    all_results = []
        
        # åˆ›å»ºä¸€ä¸ªä»…åŒ…å«æ¨èç­–ç•¥çš„ç®€æ´æ¡ç›®ï¼Œæ¸…ç†diagnosesä¸­çš„emoji
        structured_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "report_file": file_name,
            "diagnoses": [clean_emojis_for_storage(d) for d in analysis_output.get("diagnoses", [])],
            "recommended_strategies": analysis_output.get("recommended_strategies", [])
        }
        all_results.append(structured_entry)

        # ä¿å­˜æ›´æ–°åçš„JSONæ•°æ®
        os.makedirs(os.path.dirname(results_path), exist_ok=True)
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æ„åŒ–åˆ†æç»“æœå¤±è´¥: {e}")