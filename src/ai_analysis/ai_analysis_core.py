import json
import os
import time
import datetime
import json
import logging
from typing import Optional
from openai import OpenAI

# é…ç½®æ—¥å¿— - é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.setLevel(logging.INFO)  # æ”¹ä¸ºINFOçº§åˆ«å‡å°‘æ—¥å¿—é‡
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

class DataAnalyzer:
    def __init__(self, client, config, root_dir: str):
        """åˆå§‹åŒ–æ—¶æ¥æ”¶é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"""
        self.client = client
        self.config = config
        self.root_dir = root_dir
        
        # --- æ‰€æœ‰è·¯å¾„éƒ½åŸºäº root_dir æ„å»º ---
        self.data_storage_path = os.path.join(self.root_dir, config['data_storage']['file_path'])
        self.hourly_log_path = os.path.join(self.root_dir, 'data', 'storage', 'hourly_data_log.json')
        self.strategy_library_path = os.path.join(self.root_dir, 'src', 'ai_analysis', 'strategy_library.json')
        self.speech_data_path = os.path.join(self.root_dir, config.get('speech_data', {}).get('file_path', 'text/latest_two_cleaned.json'))
        
        self.ensure_data_file_exists()

    def ensure_data_file_exists(self):
        """ç¡®ä¿æ•°æ®å­˜å‚¨æ–‡ä»¶å­˜åœ¨å¹¶åˆå§‹åŒ–"""
        try:
            if not os.path.exists(self.data_storage_path):
                # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
                os.makedirs(os.path.dirname(self.data_storage_path), exist_ok=True)
                with open(self.data_storage_path, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ–æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _load_strategy_library(self):
        """æ–°å¢æ–¹æ³•ï¼šåŠ è½½æˆ˜æœ¯ä¸è¯æœ¯åº“"""
        try:
            with open(self.strategy_library_path, 'r', encoding='utf-8') as f:
                return json.load(f).get('strategies', [])
        except FileNotFoundError:
            logger.error(f"ç­–ç•¥åº“æ–‡ä»¶æœªæ‰¾åˆ°: {self.strategy_library_path}")
            return []
        except Exception as e:
            logger.error(f"åŠ è½½ç­–ç•¥åº“å¤±è´¥: {str(e)}")
            return []

    def _get_diagnosis_from_ai(self, current_data, previous_data, speech_content, special_variables: Optional[str] = None):
        """ä¿®æ”¹æ–¹æ³•ï¼šæ”¹ä¸ºç›´æ¥ä»AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤ï¼Œå¹¶å¼ºåˆ¶å…¶å¿…é¡»è¿”å›å†…å®¹"""
        
        # ä¿®å¤ï¼šç¡®ä¿ä¼ é€’ç»™AIçš„æ˜¯çº¯å‡€çš„æŒ‡æ ‡æ•°æ®ï¼Œè€Œä¸æ˜¯åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´å¯¹è±¡
        current_pure_data = current_data.get('data', current_data)  # å¦‚æœæ˜¯å®Œæ•´å¯¹è±¡ï¼Œæå–dataå­—æ®µ
        previous_pure_data = previous_data.get('data', previous_data) if previous_data else {}
        
        # å¼ºåˆ¶è®°å½•ä¼ é€’ç»™è¯Šæ–­AIçš„åŸå§‹æ•°æ®
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯Šæ–­AIçš„å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}")
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯Šæ–­AIçš„å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}")
        
        # æ„å»ºå˜é‡ä¿¡æ¯éƒ¨åˆ†
        variables_prompt_part = ""
        if special_variables:
            variables_prompt_part = f"""
        **ä»Šæ—¥ç‰¹æ®Šå˜é‡**:
        {special_variables}
        ---
        """

        # æ„å»ºå®Œæ•´çš„Promptï¼Œè¦æ±‚AIåŒæ—¶æä¾›è¯Šæ–­å’Œå…·ä½“æˆ˜æœ¯æŒ‡ä»¤
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç›´æ’­æ•°æ®åˆ†æå¸ˆå’Œé”€å”®ç­–ç•¥ä¸“å®¶ã€‚è¯·å¯¹æ¯”ä»¥ä¸‹å½“å‰å°æ—¶å’Œä¸Šä¸€å°æ—¶çš„æ•°æ®ï¼Œä»¥åŠå½“å‰å°æ—¶çš„ä¸»æ’­è¯æœ¯ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯æ‰¾å‡ºæ ¸å¿ƒé—®é¢˜å¹¶æä¾›å…·ä½“çš„æˆ˜æœ¯æŒ‡ä»¤æ¥æ”¹å–„é—®é¢˜ã€‚

        **é‡è¦è§„åˆ™ï¼šå¿…é¡»æä¾›è‡³å°‘ä¸€æ¡æˆ˜æœ¯æŒ‡ä»¤ã€‚å¦‚æœæ•°æ®è¡¨ç°å¹³ç¨³æˆ–ä¼˜ç§€ï¼Œè¯·æä¾›ä¸€æ¡"ç»´æŒä¼˜åŠ¿"æˆ–"é”¦ä¸Šæ·»èŠ±"çš„é¼“åŠ±æ€§æŒ‡ä»¤ã€‚**
        
        {variables_prompt_part}
        å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}
        å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}
        è¯æœ¯å†…å®¹: {speech_content}
        
        é¦–å…ˆè¯Šæ–­é—®é¢˜ï¼Œæ‰¾å‡ºä»¥ä¸‹å¸¸è§é—®é¢˜ä¸­å­˜åœ¨çš„1-3ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¯·è¯Šæ–­ä¸ºâ€œæ•°æ®è¡¨ç°å¹³ç¨³â€ã€‚
        - è½¬åŒ–ç‡ä¸‹é™/è½¬åŒ–ç‡ä½
        - äº’åŠ¨ç‡ä½/è¯„è®ºå°‘/ç‚¹èµå°‘/åœºå­å†·
        - å®¢å•ä»·ä½/å¤§ä»¶è½¬åŒ–ä¸è¶³/ä»·å€¼å¡‘é€ ä¸è¶³
        - ç”¨æˆ·çŠ¹è±«/ä¸´é—¨ä¸€è„š
        - æ•°æ®è¡¨ç°å¹³ç¨³
        
        ç„¶åï¼Œå¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸€ä¸ªå…·ä½“çš„æˆ˜æœ¯æŒ‡ä»¤ï¼ŒåŒ…æ‹¬:
        1. æˆ˜æœ¯åç§°ï¼šç®€çŸ­æœ‰åŠ›çš„æ ‡é¢˜
        2. ç›®æ ‡ï¼šè¿™ä¸ªæˆ˜æœ¯æƒ³è¦è¾¾æˆçš„æ•ˆæœ
        3. å…·ä½“æŒ‡ä»¤ï¼šè¯¦ç»†çš„æ‰§è¡Œæ–¹æ³•ï¼ŒåŒ…æ‹¬è¯æœ¯ç¤ºä¾‹
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šæˆ–æ–‡æœ¬:
        {{
          "diagnoses": ["è¯Šæ–­å‡ºçš„é—®é¢˜1"],
          "strategies": [
            {{
              "id": "ai-gen-{int(time.time())}",
              "name": "æˆ˜æœ¯åç§°1",
              "goal": "æˆ˜æœ¯ç›®æ ‡1",
              "instruction": "è¯¦ç»†æŒ‡ä»¤å†…å®¹1ï¼ŒåŒ…æ‹¬å…·ä½“è¯æœ¯ç¤ºä¾‹"
            }}
          ]
        }}
        """
        try:
            logger.info("æ­£åœ¨è°ƒç”¨è±†åŒ…AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤...")
            response = self.client.chat.completions.create(
                model=self.config['douban_api']['model_name'],
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"} # å¼€å¯JSONæ¨¡å¼ä»¥ç¡®ä¿æ ¼å¼æ­£ç¡®
            )
            ai_response_content = response.choices[0].message.content
            logger.info(f"æˆåŠŸä»AIè·å–åˆ°å“åº”: {ai_response_content}")
            # ç¡®ä¿è¿”å›çš„æ˜¯JSONæ ¼å¼
            return json.loads(ai_response_content)
        except Exception as e:
            logger.error(f"ä»AIè·å–è¯Šæ–­å’Œæˆ˜æœ¯æŒ‡ä»¤å¤±è´¥: {e}", exc_info=True)
            # åœ¨APIå¤±è´¥æ—¶è¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„é»˜è®¤ç»“æœ
            return {
                "diagnoses": ["AIåˆ†æå¼‚å¸¸"], 
                "strategies": [{
                    "id": "error-fallback",
                    "name": "AIåˆ†ææœåŠ¡å‡ºç°é—®é¢˜",
                    "goal": "æç¤ºç”¨æˆ·æ£€æŸ¥åå°æœåŠ¡",
                    "instruction": f"è°ƒç”¨AIè¿›è¡Œåˆ†ææ—¶å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥åå°æ—¥å¿—ã€‚é”™è¯¯è¯¦æƒ…: {str(e)}"
                }]
            }

    # ç§»é™¤ä¸å†éœ€è¦çš„åŒ¹é…æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„æˆ˜æœ¯
    # def _match_strategies(self, diagnoses_keywords, strategy_library):
    #    """æ–°å¢æ–¹æ³•ï¼šç¬¬äºŒæ­¥ - åŒ¹é…ç­–ç•¥ï¼Œæ­¤æ­¥ä¸è°ƒç”¨AI"""
    #    matched_strategies = []
    #    if not diagnoses_keywords:
    #        return matched_strategies
    #        
    #    for keyword in diagnoses_keywords:
    #        for strategy in strategy_library:
    #            if keyword in strategy.get('triggers', []):
    #                # é¿å…é‡å¤æ·»åŠ åŒä¸€ä¸ªç­–ç•¥
    #                if strategy not in matched_strategies:
    #                    matched_strategies.append(strategy)
    #    return matched_strategies

    def load_speech_data(self):
        """åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®"""
        try:
            if not os.path.exists(self.speech_data_path):
                logger.warning(f"ä¸»æ’­è¯æœ¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.speech_data_path}")
                return []

            with open(self.speech_data_path, 'r', encoding='utf-8') as f:
                speech_data = json.load(f)
                if not isinstance(speech_data, list):
                    logger.warning("ä¸»æ’­è¯æœ¯æ•°æ®æ ¼å¼åº”ä¸ºæ•°ç»„ï¼Œå·²è½¬æ¢ä¸ºå•å…ƒç´ æ•°ç»„")
                    speech_data = [speech_data]
                return speech_data
        except Exception as e:
            logger.error(f"åŠ è½½ä¸»æ’­è¯æœ¯æ•°æ®å¤±è´¥: {str(e)}")
            return []

    def find_matching_speech(self, date_str, time_range):
        """æ ¹æ®æ—¥æœŸå’Œæ—¶é—´æ®µæŸ¥æ‰¾åŒ¹é…çš„ä¸»æ’­è¯æœ¯"""
        speech_data = self.load_speech_data()
        for entry in speech_data:
            # æ ‡å‡†åŒ–æ—¥æœŸå’Œæ—¶é—´æ®µæ ¼å¼è¿›è¡ŒåŒ¹é…
            entry_date = entry.get('æ—¥æœŸ', '')
            # æ ‡å‡†åŒ–æ—¥æœŸå’Œæ—¶é—´æ®µæ ¼å¼è¿›è¡ŒåŒ¹é…
            entry_date = entry.get('æ—¥æœŸ', '')
            original_time = entry.get('å°æ—¶', '')
            # å¤„ç†ä¸åŒæ ¼å¼çš„æ—¶é—´æ®µè¡¨ç¤º
            if 'ç‚¹' in original_time:
                # å¤„ç†'10ç‚¹-11ç‚¹'æ ¼å¼
                start_hour = original_time.split('-')[0].replace('ç‚¹', '').strip()
                entry_time = f"{start_hour}:00-{int(start_hour)+1}:00"
            else:
                # ç›´æ¥ä½¿ç”¨ç°æœ‰æ ¼å¼å¦‚'10:00-11:00'
                entry_time = original_time
            if entry_date == date_str and entry_time == time_range:
                return entry.get('text', '')
        logger.info(f"æœªæ‰¾åˆ°åŒ¹é…çš„ä¸»æ’­è¯æœ¯æ•°æ®: {date_str} {time_range}")
        return ""
    
    def load_data_from_csv(self):
        """ä» new_format_data.csv æ–‡ä»¶ä¸­è¯»å–æœ€åä¸¤è¡Œæ•°æ®ï¼ˆä¿®å¤ï¼šç›´æ¥ä»æ–‡ä»¶è¯»å–çœŸæ­£çš„æœ€åä¸¤è¡Œï¼‰"""
        try:
            csv_path = os.path.join(self.root_dir, 'data', 'baseline_data', 'new_format_data.csv')
            if not os.path.exists(csv_path):
                logger.warning(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
                return None, None
            
            # ä¿®å¤ï¼šç›´æ¥ä»æ–‡ä»¶è¯»å–æœ€åä¸¤è¡Œï¼Œé¿å…pandasè·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
            import pandas as pd
            
            # é¦–å…ˆè¯»å–æ–‡ä»¶çš„æ‰€æœ‰è¡Œæ¥è·å–çœŸæ­£çš„æœ€åä¸¤è¡Œ
            with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            if len(lines) < 3:  # è‡³å°‘éœ€è¦å¤´éƒ¨+2è¡Œæ•°æ®
                logger.warning("CSVæ–‡ä»¶è¡Œæ•°ä¸è¶³")
                return None, None
            
            # è·å–å¤´éƒ¨å’Œæœ€åä¸¤è¡Œ
            header_line = lines[0].strip()
            last_line = lines[-1].strip()
            second_last_line = lines[-2].strip()
            
            logger.info(f"CSVæ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
            logger.info(f"çœŸæ­£çš„æœ€åä¸€è¡Œ(ç¬¬{len(lines)}è¡Œ): {last_line[:100]}...")
            logger.info(f"çœŸæ­£çš„å€’æ•°ç¬¬äºŒè¡Œ(ç¬¬{len(lines)-1}è¡Œ): {second_last_line[:100]}...")
            
            # è§£æå¤´éƒ¨è·å–åˆ—å
            headers = [h.strip() for h in header_line.split(',')]
            logger.info(f"CSVå¤´éƒ¨åˆ—æ•°: {len(headers)}")
            
            # è§£ææœ€åä¸¤è¡Œæ•°æ®
            def parse_csv_line(line, headers):
                """è§£æCSVè¡Œï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜"""
                values = [v.strip() for v in line.split(',')]
                # å¦‚æœå­—æ®µæ•°ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–å¡«å……
                if len(values) > len(headers):
                    logger.warning(f"è¡Œå­—æ®µæ•°({len(values)})è¶…è¿‡å¤´éƒ¨å­—æ®µæ•°({len(headers)})ï¼Œæˆªæ–­å¤šä½™å­—æ®µ")
                    values = values[:len(headers)]
                elif len(values) < len(headers):
                    logger.warning(f"è¡Œå­—æ®µæ•°({len(values)})å°‘äºå¤´éƒ¨å­—æ®µæ•°({len(headers)})ï¼Œå¡«å……ç©ºå€¼")
                    values.extend([''] * (len(headers) - len(values)))
                
                return dict(zip(headers, values))
            
            current_data = parse_csv_line(last_line, headers)
            previous_data = parse_csv_line(second_last_line, headers)
            
            # è®°å½•è¯»å–çš„æ•°æ®ç”¨äºè°ƒè¯•
            logger.info(f"è§£æåçš„å½“å‰æ•°æ®æ—¥æœŸ: {current_data.get('æ—¥æœŸ', 'N/A')} {current_data.get('å°æ—¶', 'N/A')}")
            logger.info(f"è§£æåçš„å†å²æ•°æ®æ—¥æœŸ: {previous_data.get('æ—¥æœŸ', 'N/A')} {previous_data.get('å°æ—¶', 'N/A')}")
            
            # è¯¦ç»†è®°å½•å…³é”®æŒ‡æ ‡çš„CSVåŸå§‹å€¼
            key_indicators = ['æ¶ˆè€—', 'æ•´ä½“GMV', 'æ•´ä½“ROI']
            for indicator in key_indicators:
                if indicator in current_data:
                    logger.info(f"CSVå½“å‰æ•°æ®(çœŸæ­£ç¬¬{len(lines)}è¡Œ) {indicator}: {current_data[indicator]}")
                if indicator in previous_data:
                    logger.info(f"CSVå†å²æ•°æ®(çœŸæ­£ç¬¬{len(lines)-1}è¡Œ) {indicator}: {previous_data[indicator]}")
            
            return current_data, previous_data
                
        except Exception as e:
            logger.error(f"ä»CSVæ–‡ä»¶è¯»å–æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None, None
    
    def load_speech_from_json(self, target_date, target_hour):
        """ä» latest_two_cleaned.json ä¸­æ ¹æ®æ—¥æœŸå’Œå°æ—¶åŒ¹é…è¯æœ¯å†…å®¹"""
        try:
            json_path = os.path.join(self.root_dir, 'text', 'latest_two_cleaned.json')
            if not os.path.exists(json_path):
                logger.warning(f"è¯æœ¯JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
                return ""
            
            with open(json_path, 'r', encoding='utf-8') as f:
                speech_data = json.load(f)
            
            # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(speech_data, list):
                speech_data = [speech_data] if speech_data else []
            
            # æŸ¥æ‰¾åŒ¹é…çš„è¯æœ¯å†…å®¹
            for entry in speech_data:
                entry_date = entry.get('æ—¥æœŸ', '')
                entry_hour = entry.get('å°æ—¶', '')
                
                # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼è¿›è¡ŒåŒ¹é…
                if entry_date == target_date and entry_hour == target_hour:
                    return entry.get('text', '')
            
            logger.info(f"æœªæ‰¾åˆ°åŒ¹é…çš„è¯æœ¯æ•°æ®: {target_date} {target_hour}")
            return ""
            
        except Exception as e:
            logger.error(f"ä»JSONæ–‡ä»¶è¯»å–è¯æœ¯å¤±è´¥: {e}", exc_info=True)
            return ""

    def get_previous_hour_data(self):
        """è·å–ä¸Šä¸€å°æ—¶çš„æ•°æ®"""
        try:
            # ä¿®å¤ï¼šä»æ–°çš„hourly_log_pathè¯»å–æ•°æ®ï¼Œè€Œä¸æ˜¯æ—§çš„data_storage_path
            if not os.path.exists(self.hourly_log_path):
                logger.warning(f"æ‰¾ä¸åˆ°å°æ—¶æ•°æ®æ—¥å¿—æ–‡ä»¶: {self.hourly_log_path}")
                return None
                
            with open(self.hourly_log_path, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
                
            if len(all_data) >= 2:
                return all_data[-2]  # è¿”å›å€’æ•°ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆä¸Šä¸€å°æ—¶ï¼‰
            elif len(all_data) == 1:
                logger.info("åªæœ‰ä¸€æ¡å†å²è®°å½•ï¼Œæ— æ³•è·å–ä¸Šä¸€å°æ—¶æ•°æ®")
                return None
            else:
                logger.warning("å†å²æ•°æ®æ—¥å¿—ä¸ºç©º")
                return None
        except Exception as e:
            logger.error(f"è·å–ä¸Šä¸€å°æ—¶æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return None

    def analyze_with_ai(self, current_data, previous_data, speech_content):
        """
        æ­¤æ–¹æ³•å°†è¢«åºŸå¼ƒï¼Œå…¶é€»è¾‘è¢«æ–°çš„ process_hourly_analysis æµç¨‹å–ä»£ã€‚
        ä¸ºä¿æŒå…¼å®¹ï¼Œæš‚æ—¶ä¿ç•™ä½†ä¸å†ä½¿ç”¨ã€‚
        """
        # æ„å»ºæç¤ºè¯
        # è·å–åˆ†æé˜ˆå€¼å¹¶è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        threshold_percent = self.config['analysis']['threshold'] * 100
        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        prompt = self.config['analysis']['prompt'].format(
            current_data=json.dumps(current_data['data'], ensure_ascii=False),
            previous_data=json.dumps(previous_data['data'], ensure_ascii=False),
            speech_content=speech_content,
            threshold=threshold_percent,
            current_time=current_time
        )
        
        # è°ƒç”¨è±†åŒ…APIè¿›è¡Œåˆ†æ
        response = self.client.chat.completions.create(
            model=self.config['douban_api']['model_name'],
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    def _generate_detailed_report_with_ai(self, current_data, previous_data, speech_content):
        """
        æ–°å¢æ–¹æ³•ï¼šä¸“é—¨ç”¨äºç”Ÿæˆæ—§ç‰ˆçš„ã€åŒ…å«è¯¦ç»†æ•°æ®è¡¨æ ¼å’Œåˆ†æçš„MarkdownæŠ¥å‘Šã€‚
        ä¿®å¤æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼šç¡®ä¿AIä½¿ç”¨çš„æ•°æ®ä¸CSVæ–‡ä»¶ä¸­çš„åŸå§‹æ•°æ®å®Œå…¨ä¸€è‡´ã€‚
        ä¿®å¤æŒ‡æ ‡æ˜ å°„é—®é¢˜ï¼šåŠ¨æ€è·å–é£ä¹¦æ•°æ®æºçš„çœŸå®æŒ‡æ ‡åç§°ï¼Œç¡®ä¿AIä½¿ç”¨æ­£ç¡®çš„æŒ‡æ ‡åç§°ã€‚
        """
        threshold_percent = self.config['analysis']['threshold'] * 100
        current_time_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ä¿®å¤ï¼šç¡®ä¿ä¼ é€’ç»™AIçš„æ˜¯çº¯å‡€çš„æŒ‡æ ‡æ•°æ®ï¼Œè€Œä¸æ˜¯åŒ…å«å…ƒæ•°æ®çš„å®Œæ•´å¯¹è±¡
        current_pure_data = current_data.get('data', current_data)  # å¦‚æœæ˜¯å®Œæ•´å¯¹è±¡ï¼Œæå–dataå­—æ®µ
        previous_pure_data = previous_data.get('data', previous_data) if previous_data else {}
        
        # å¼ºåˆ¶è®°å½•ä¼ é€’ç»™AIçš„åŸå§‹æ•°æ®
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯¦ç»†æŠ¥å‘ŠAIçš„å½“å‰æ•°æ®: {json.dumps(current_pure_data, ensure_ascii=False)}")
        logger.info(f"ğŸ” ä¼ é€’ç»™è¯¦ç»†æŠ¥å‘ŠAIçš„å†å²æ•°æ®: {json.dumps(previous_pure_data, ensure_ascii=False)}")
        
        # æ•°æ®ä¸€è‡´æ€§ä¿®å¤ï¼šæ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®ï¼Œç¡®ä¿ä¸CSVåŸå§‹æ•°æ®å®Œå…¨ä¸€è‡´
        def clean_data_for_ai(data_dict):
            """æ¸…ç†æ•°æ®å­—å…¸ï¼Œç§»é™¤NaNå€¼å’Œéæ•°å€¼æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§"""
            cleaned = {}
            for key, value in data_dict.items():
                # è·³è¿‡éæ•°å€¼åˆ—
                if key in ['æ—¥æœŸ', 'å°æ—¶', 'ä¸»æ’­', 'åœºæ§', 'åœºæ¬¡']:
                    cleaned[key] = str(value) if value is not None else ''
                else:
                    # å¤„ç†æ•°å€¼åˆ—
                    if value is None or str(value).lower() in ['nan', 'null', '']:
                        cleaned[key] = 0
                    else:
                        try:
                            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œä¿æŒåŸå§‹ç²¾åº¦
                            if isinstance(value, (int, float)):
                                cleaned[key] = float(value)
                            else:
                                # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„æ•°å€¼
                                str_value = str(value).strip()
                                if str_value == '' or str_value.lower() in ['nan', 'null', 'none']:
                                    cleaned[key] = 0
                                else:
                                    cleaned[key] = float(str_value)
                        except (ValueError, TypeError):
                            logger.warning(f"æ— æ³•è½¬æ¢æ•°å€¼: {key}={value}, è®¾ç½®ä¸º0")
                            cleaned[key] = 0
            return cleaned
        
        # æ¸…ç†å½“å‰å’Œå†å²æ•°æ®
        current_clean_data = clean_data_for_ai(current_pure_data)
        previous_clean_data = clean_data_for_ai(previous_pure_data)
        
        # è®°å½•æ•°æ®æ¸…ç†æ—¥å¿—å’Œå…³é”®æŒ‡æ ‡å¯¹æ¯”
        logger.info(f"æ•°æ®æ¸…ç†å®Œæˆ - å½“å‰æ•°æ®æ¡ç›®æ•°: {len(current_clean_data)}, å†å²æ•°æ®æ¡ç›®æ•°: {len(previous_clean_data)}")
        
        # è¯¦ç»†è®°å½•å…³é”®æŒ‡æ ‡çš„åŸå§‹å€¼å’Œæ¸…ç†åçš„å€¼
        key_indicators = ['æ¶ˆè€—', 'æ•´ä½“GMV', 'æ•´ä½“ROI']
        for indicator in key_indicators:
            if indicator in current_pure_data and indicator in current_clean_data:
                logger.info(f"å½“å‰æ•°æ® {indicator}: åŸå§‹å€¼={current_pure_data[indicator]}, æ¸…ç†å={current_clean_data[indicator]}")
            if indicator in previous_pure_data and indicator in previous_clean_data:
                logger.info(f"å†å²æ•°æ® {indicator}: åŸå§‹å€¼={previous_pure_data[indicator]}, æ¸…ç†å={previous_clean_data[indicator]}")
        
        # ä¿®å¤æŒ‡æ ‡æ˜ å°„ï¼šåŠ¨æ€ç”ŸæˆæŒ‡æ ‡è¡¨æ ¼è¡Œï¼Œä½¿ç”¨é£ä¹¦æ•°æ®æºçš„çœŸå®æŒ‡æ ‡åç§°
        def generate_indicator_table_rows(data_dict):
            """æ ¹æ®å®é™…æ•°æ®åŠ¨æ€ç”ŸæˆæŒ‡æ ‡è¡¨æ ¼è¡Œ"""
            table_rows = []
            # æ’é™¤éæŒ‡æ ‡åˆ—
            excluded_columns = ['æ—¥æœŸ', 'å°æ—¶', 'ä¸»æ’­', 'åœºæ§', 'åœºæ¬¡']
            for key in data_dict.keys():
                if key not in excluded_columns:
                    table_rows.append(f"| {key} |        |          |            |      |      |")
            return "\n".join(table_rows)
        
        # ç”ŸæˆåŠ¨æ€æŒ‡æ ‡è¡¨æ ¼
        indicator_table_rows = generate_indicator_table_rows(current_clean_data)
        
        # ä½¿ç”¨ä¿®å¤åçš„Promptæ¨¡æ¿ï¼ŒåŠ¨æ€æ’å…¥çœŸå®æŒ‡æ ‡åç§°
        prompt = f"""åˆ†æä»¥ä¸‹ä¸¤ä¸ªå°æ—¶çš„ç›´æ’­æ•°æ®å¯¹æ¯”å’Œä¸»æ’­è¯æœ¯ï¼Œæ£€æµ‹æ˜¯å¦å­˜åœ¨å¼‚å¸¸æ³¢åŠ¨ï¼š

ã€å½“å‰å°æ—¶æ•°æ®ã€‘
{json.dumps(current_clean_data, ensure_ascii=False, indent=2)}

ã€ä¸Šä¸€å°æ—¶æ•°æ®ã€‘
{json.dumps(previous_clean_data, ensure_ascii=False, indent=2)}

ã€ä¸»æ’­è¯æœ¯æ‘˜è¦ã€‘
{speech_content}

è¯·æ‰§è¡Œä»¥ä¸‹æ·±åº¦åˆ†æï¼ˆä¸¥æ ¼æŒ‰æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿å†…å®¹è¯¦å®ï¼‰ï¼š
1. ã€å…¨é¢æŒ‡æ ‡åˆ†æã€‘å¯¹æ¯”æ‰€æœ‰æŒ‡æ ‡å·®å¼‚ï¼Œè®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰ï¼Œåˆ†æç»Ÿè®¡æ˜¾è‘—æ€§

2. ã€å¼‚å¸¸æ£€æµ‹ã€‘éµå¾ªä»¥ä¸‹æå…¶ä¸¥æ ¼çš„åˆ¤æ–­è§„åˆ™ï¼š
   - æ‰€æœ‰æŒ‡æ ‡ä¸Šæ¶¨ï¼Œæ— è®ºä¸Šæ¶¨å¤šå°‘ï¼Œå¿…é¡»æ ‡è®°ä¸ºğŸŸ¢æ­£å¸¸
   - æ‰€æœ‰æŒ‡æ ‡ä¸‹é™ä½†å¹…åº¦å°äº{threshold_percent}%ï¼Œå¿…é¡»æ ‡è®°ä¸ºğŸŸ¢æ­£å¸¸
   - ä»…å½“æŒ‡æ ‡ä¸‹é™å¹…åº¦è¶…è¿‡{threshold_percent}%æ—¶ï¼Œæ‰èƒ½æ ‡è®°ä¸ºğŸ”´å¼‚å¸¸
   - ç‰¹åˆ«æ³¨æ„ï¼šä¸Šæ¶¨çš„æŒ‡æ ‡ç»å¯¹ä¸èƒ½æ ‡è®°ä¸ºå¼‚å¸¸ï¼Œå³ä½¿ä¸Šæ¶¨å¹…åº¦å¾ˆå¤§

3. ã€äº§å“æåŠåˆ†æã€‘
   - æå–æ‰€æœ‰æåŠçš„äº§å“åç§°åŠæåŠæ¬¡æ•°ï¼Œç‰¹åˆ«å…³æ³¨'PWUæ´—è¡£ç•™é¦™ç 'ã€'ç•™é¦™ç 'ã€'æ´—è¡£ç 'ç­‰å…³é”®äº§å“
   - åˆ†æå„äº§å“å…³è”çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/ä¸­æ€§/è´Ÿé¢ï¼‰
   - å…³è”äº§å“æåŠä¸é”€å”®è½¬åŒ–çš„å…³ç³»

4. ã€è¯æœ¯æ·±åº¦åˆ†æã€‘
   - æå–å…³é”®é”€å”®è¯æœ¯ï¼ˆä¿ƒé”€ç­–ç•¥/äº§å“å–ç‚¹/äº’åŠ¨å¼•å¯¼ï¼‰
   - é‡åŒ–åˆ†æè¯æœ¯ç‰¹å¾ï¼ˆåŸºäºæä¾›çš„ã€å…³é”®è¯ç»Ÿè®¡ã€‘å’Œã€æƒ…æ„Ÿåˆ†æã€‘ç»“æœï¼‰
   - å»ºç«‹è¯æœ¯ä¸æŒ‡æ ‡å…³è”æ€§ï¼ˆå¦‚ï¼š"é™æ—¶ä¼˜æƒ "è¯æœ¯ä¸è½¬åŒ–ç‡å…³ç³»ï¼‰

5. ã€æ ¹å› è¯Šæ–­ã€‘ç»“åˆæ•°æ®ä¸è¯æœ¯æä¾›3-5ä¸ªå¯èƒ½åŸå› ï¼Œæ¯ä¸ªåŸå› éœ€åŒ…å«ï¼š
   - å…·ä½“æ•°æ®è¯æ®ï¼ˆæŒ‡æ ‡å˜åŒ–å€¼ï¼‰
   - ç›¸å…³è¯æœ¯ç‰‡æ®µï¼ˆç›´æ¥å¼•ç”¨ï¼‰
   - å› æœå…³ç³»è§£é‡Š

6. ã€è¶‹åŠ¿é¢„æµ‹ã€‘åŸºäºå½“å‰æ•°æ®å’Œè¯æœ¯æ•ˆæœé¢„æµ‹ä¸‹ä¸€å°æ—¶å¯èƒ½è¶‹åŠ¿

7. ã€é¢„è­¦ä¿¡æ¯ã€‘å¦‚æœ‰å¼‚å¸¸ï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†çº§ï¼ˆP0-P2ï¼‰

è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨å¢å¼ºMarkdownæ ¼å¼ï¼Œç¡®ä¿è§†è§‰æ¸…æ™°ï¼‰ï¼š
## ğŸ“Š æŒ‡æ ‡å˜åŒ–åˆ†æ
**é‡è¦æç¤ºï¼šå¿…é¡»æ˜¾ç¤ºæ‰€æœ‰æŒ‡æ ‡çš„å¯¹æ¯”ï¼Œä½¿ç”¨é£ä¹¦æ•°æ®æºä¸­çš„çœŸå®æŒ‡æ ‡åç§°ï¼Œä¸èƒ½çœç•¥ä»»ä½•æŒ‡æ ‡**
| æŒ‡æ ‡åç§° | å½“å‰å€¼ | ä¸Šå°æ—¶å€¼ | å˜åŒ–ç™¾åˆ†æ¯” | è¶‹åŠ¿ | çŠ¶æ€ |
|----------|--------|----------|------------|------|------|
{indicator_table_rows}
> **çŠ¶æ€è¯´æ˜**ï¼šğŸ”´ å¼‚å¸¸ï¼ˆä¸‹é™è¶…è¿‡{threshold_percent}%ï¼‰ | ğŸŸ¢ æ­£å¸¸ï¼ˆä¸Šæ¶¨æˆ–ä¸‹é™ä¸è¶³{threshold_percent}%ï¼‰

## ğŸ” äº§å“æåŠåˆ†æ
| äº§å“åç§° | æåŠæ¬¡æ•° | æƒ…æ„Ÿå€¾å‘ | ç›¸å…³æŒ‡æ ‡å˜åŒ– |
|----------|----------|----------|------------|
| PWUæ´—è¡£ç•™é¦™ç  | 12     | æ­£é¢     | è½¬åŒ–ç‡+2.5% |
| ...      | ...      | ...      | ...        |

## âš ï¸ å¼‚å¸¸æŒ‡æ ‡é¢„è­¦
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„åµŒå¥—åˆ—è¡¨æ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨4ä¸ªç©ºæ ¼è¿›è¡Œç¼©è¿›åˆ›å»ºå­åˆ—è¡¨:
- **æŒ‡æ ‡åç§° (å˜åŒ–ç™¾åˆ†æ¯”)**:
    - **åŸå› åˆ†æ**: [AIåˆ†æçš„åŸå› ]
    - **æ•°æ®è¯æ®**: [å¼•ç”¨çš„å…·ä½“æ•°æ®]
    - **è¯æœ¯è¯æ®**: [å¼•ç”¨çš„ç›¸å…³è¯æœ¯]

## ğŸ’¡ ä¼˜åŒ–å»ºè®®
1. **æ•°æ®éªŒè¯**: æ£€æŸ¥[å…·ä½“æŒ‡æ ‡]æ•°æ®é‡‡é›†é€»è¾‘ï¼Œç¡®ä¿å‡†ç¡®æ€§
2. **è¯æœ¯ä¼˜åŒ–**: å°†[å½“å‰è¯æœ¯é—®é¢˜]è°ƒæ•´ä¸º[å»ºè®®è¯æœ¯ç¤ºä¾‹]ï¼ˆé¢„è®¡æå‡[é¢„æœŸæ•ˆæœ]ï¼‰
3. **æ•ˆæœè·Ÿè¸ª**: é€šè¿‡å¯¹æ¯”[éªŒè¯æŒ‡æ ‡]åœ¨ä¸‹ä¸€ä¸ªå°æ—¶å†…çš„å˜åŒ–éªŒè¯ä¼˜åŒ–æ•ˆæœ

> **åˆ†æå‘¨æœŸ**ï¼š{current_time_str} | **æ•°æ®æ¥æº**ï¼šé£ä¹¦è¡¨æ ¼"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.config['douban_api']['model_name'],
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯¦ç»†AIåˆ†ææŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return f"# AIåˆ†æé”™è¯¯\n\nåœ¨ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}"


    def process_hourly_analysis(self, special_variables: Optional[str] = None) -> dict:
        """
        é‡æ„åçš„æ ¸å¿ƒæ–¹æ³•ï¼Œå¤„ç†æ¯å°æ—¶æ•°æ®åˆ†ææµç¨‹ã€‚
        ä»CSVæ–‡ä»¶è¯»å–æ•°æ®ï¼Œä»JSONæ–‡ä»¶åŒ¹é…è¯æœ¯å†…å®¹ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å«ç»“æ„åŒ–æ•°æ®å’ŒMarkdownæŠ¥å‘Šçš„å­—å…¸ã€‚
        """
        try:
            # ä»CSVæ–‡ä»¶è¯»å–æœ€åä¸¤è¡Œæ•°æ®
            current_data, previous_data = self.load_data_from_csv()
            
            if not current_data:
                message = "æ— æ³•ä»CSVæ–‡ä»¶è¯»å–æ•°æ®"
                logger.error(message)
                return {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "diagnoses": ["æ•°æ®è¯»å–å¤±è´¥"],
                    "recommended_strategies": [],
                    "report_markdown": f"# {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')} ç›´æ’­å¤ç›˜AIæŒ‡ä»¤\n\n{message}"
                }
            
            # è·å–å½“å‰æ•°æ®çš„æ—¥æœŸå’Œå°æ—¶ä¿¡æ¯
            current_date = current_data.get('æ—¥æœŸ', '')
            current_hour = current_data.get('å°æ—¶', '')
            
            # ä»JSONæ–‡ä»¶åŒ¹é…å½“å‰å°æ—¶çš„è¯æœ¯å†…å®¹
            current_speech_content = self.load_speech_from_json(current_date, current_hour)
            
            # è·å–ä¸Šä¸€å°æ—¶çš„è¯æœ¯å†…å®¹ï¼ˆå¦‚æœæœ‰ä¸Šä¸€å°æ—¶æ•°æ®ï¼‰
            previous_speech_content = ""
            if previous_data:
                previous_date = previous_data.get('æ—¥æœŸ', '')
                previous_hour = previous_data.get('å°æ—¶', '')
                previous_speech_content = self.load_speech_from_json(previous_date, previous_hour)
            
            logger.info(f"å½“å‰æ•°æ®: {current_date} {current_hour}")
            logger.info(f"å½“å‰è¯æœ¯å†…å®¹é•¿åº¦: {len(current_speech_content)}")
            if previous_data:
                logger.info(f"ä¸Šä¸€å°æ—¶æ•°æ®: {previous_date} {previous_hour}")
                logger.info(f"ä¸Šä¸€å°æ—¶è¯æœ¯å†…å®¹é•¿åº¦: {len(previous_speech_content)}")

            if not previous_data:
                # å³ä½¿æ²¡æœ‰å†å²æ•°æ®ï¼Œä¹Ÿè¿”å›æ ‡å‡†æ ¼å¼
                message = "é¦–æ¬¡è¿è¡Œï¼Œæ— å†å²æ•°æ®å¯ä¾›å¯¹æ¯”åˆ†æ"
                logger.info(message)
                return {
                    "timestamp": datetime.datetime.now().isoformat(),
                    "diagnoses": ["é¦–æ¬¡è¿è¡Œ"],
                    "recommended_strategies": [],
                    "report_markdown": f"# {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')} ç›´æ’­å¤ç›˜AIæŒ‡ä»¤\n\n{message}"
                }

            # æ„å»ºæ•°æ®ç»“æ„ç”¨äºAIåˆ†æ
            current_entry = {
                'timestamp': f"{current_date} {current_hour.split('-')[0] if '-' in current_hour else current_hour}",
                'data': current_data,
                'speech_content': current_speech_content
            }
            
            previous_entry = {
                'timestamp': f"{previous_date} {previous_hour.split('-')[0] if '-' in previous_hour else previous_hour}",
                'data': previous_data,
                'speech_content': previous_speech_content
            }

            # å…³é”®æ”¹åŠ¨ï¼šé¦–å…ˆç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
            detailed_report_md = self._generate_detailed_report_with_ai(current_entry, previous_entry, current_speech_content)

            # æ·»åŠ åŠ¨æ€åŸºçº¿å¯¹æ¯”åˆ†æ
            from src.baseline.dynamic_baseline_engine import RealDataDynamicBaseline

            # åˆå§‹åŒ–åŸºçº¿å¼•æ“
            baseline_engine = RealDataDynamicBaseline(data_dir=os.path.join(self.root_dir, 'data'))
            baseline_data_path = os.path.join(self.root_dir, 'data', 'baseline_data', 'new_format_data.csv')
            if not baseline_engine.is_initialized:
                baseline_engine.initialize_system(baseline_data_path)

            # å‡†å¤‡åŸºçº¿æŸ¥è¯¢æ•°æ®
            current_time = datetime.datetime.fromisoformat(current_entry['timestamp'])
            query_data = {
                "æ˜ŸæœŸå‡ ": current_time.weekday(),
                "å°æ—¶": current_time.hour,
                "ä¸»æ’­": current_entry.get('anchor', ''),
                **current_entry['data']
            }

            # è·å–åŸºçº¿åˆ†æç»“æœ
            baseline_result = baseline_engine.real_time_diagnosis(query_data)
            
            # è°ƒè¯•ï¼šè¾“å‡ºå®Œæ•´çš„åŸºçº¿ç»“æœç»“æ„
            logger.info(f"ğŸ” å®Œæ•´åŸºçº¿ç»“æœ: {json.dumps(baseline_result, ensure_ascii=False, indent=2)}")

            # æ ¼å¼åŒ–åŸºçº¿åˆ†æç»“æœä¸ºMarkdown
            baseline_md = "\n\n---\n\n## ğŸ“Š åŠ¨æ€åŸºçº¿å¯¹æ¯”åˆ†æ\n\n"
            if 'error' in baseline_result:
                baseline_md += f"**é”™è¯¯ä¿¡æ¯**: {baseline_result['error']}\n\n"
            else:
                baseline_md += f"**åˆ†ææ—¶æ®µ**: {baseline_result['æŸ¥è¯¢æ—¶æ®µ']}\n\n"
                baseline_md += "### æŒ‡æ ‡è¯„ä¼°ç»“æœ\n\n"
                baseline_md += "| æŒ‡æ ‡åç§° | è¯„ä¼°ç»“æœ | ç³»æ•° | åŸºçº¿å€¼ | è¯„ä¼°æ–¹æ³• |\n"
                baseline_md += "|----------|----------|------|--------|----------|\n"
                for indicator, result in baseline_result['è¯„ä¼°ç»“æœ'].items():
                    # ä¿®å¤åŸºçº¿å€¼æå–é€»è¾‘
                    baseline_value = 'N/A'
                    if 'åŸºçº¿å€¼' in result:
                        baseline_value = result['åŸºçº¿å€¼']
                    elif 'åŠ¨æ€è¯¦æƒ…' in result and 'åŸºçº¿å€¼' in result['åŠ¨æ€è¯¦æƒ…']:
                        baseline_value = result['åŠ¨æ€è¯¦æƒ…']['åŸºçº¿å€¼']
                    logger.info(f"ğŸ” è°ƒè¯•åŸºçº¿å€¼æå– - æŒ‡æ ‡: {indicator}, ç»“æœ: {result}, æå–çš„åŸºçº¿å€¼: {baseline_value}")
                    
                    baseline_md += f"| {indicator} | {result['è¯„ä¼°']} | {result['ç³»æ•°']} | {baseline_value} | {result['è¯„ä¼°æ–¹æ³•']} |\n"

            # å°†åŸºçº¿åˆ†ææ·»åŠ åˆ°æŠ¥å‘Š
            detailed_report_md += baseline_md

            # 1. (è¯Šæ–­) è°ƒç”¨AIè·å–ç»“æ„åŒ–çš„è¯Šæ–­å…³é”®è¯å’Œæˆ˜æœ¯æŒ‡ä»¤
            diagnosis_result = self._get_diagnosis_from_ai(current_entry, previous_entry, current_speech_content, special_variables)
            diagnoses_keywords = diagnosis_result.get("diagnoses", [])
            matched_strategies = diagnosis_result.get("strategies", []) # ç›´æ¥ä½¿ç”¨AIç”Ÿæˆçš„æˆ˜æœ¯
            
            # 2. (æ•´åˆ) å°†æ–°çš„AIæŒ‡ä»¤è¿½åŠ åˆ°è¯¦ç»†æŠ¥å‘Šæœ«å°¾
            final_report_md = detailed_report_md
            if matched_strategies:
                instructions_md_parts = [
                    "\n\n---\n\n",
                    "## ğŸ¤– AIæˆ˜æœ¯æŒ‡ä»¤\n\n",
                    f"**AIè¯Šæ–­å‡ºçš„æ ¸å¿ƒé—®é¢˜æ˜¯**: {', '.join(diagnoses_keywords)}\n\n",
                    "**[AIæŒ‡ä»¤]** ä¸»æ’­åŠåœºæ§è¯·æ³¨æ„ï¼Œè¯·ç«‹å³æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n"
                ]
                for i, strategy in enumerate(matched_strategies, 1):
                    instructions_md_parts.append(
                        f"\n**{i}. {strategy['name']} (ç›®æ ‡: {strategy['goal']})**\n"
                        f"   - **æŒ‡ä»¤è¯¦æƒ…**: {strategy['instruction']}\n"
                    )
                final_report_md += "".join(instructions_md_parts)

            return {
                "timestamp": datetime.datetime.now().isoformat(),
                "diagnoses": diagnoses_keywords,
                "recommended_strategy_ids": [s.get('id') for s in matched_strategies], # è¿”å›ç­–ç•¥ID
                "recommended_strategies": matched_strategies, # ä¿å­˜å®Œæ•´çš„æˆ˜æœ¯æŒ‡ä»¤
                "report_markdown": final_report_md
            }
        
        except Exception as e:
            logger.error(f"å¤„ç†å°æ—¶çº§åˆ†ææ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return {
                "timestamp": datetime.datetime.now().isoformat(),
                "diagnoses": ["Error"],
                "recommended_strategies": [],
                "report_markdown": f"# åˆ†ææµç¨‹é”™è¯¯\n\nå¤„ç†æ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
            }


def save_analysis_result(analysis_output: dict, root_dir: str):
    """
    ä¿å­˜åˆ†æç»“æœä¸ºMarkdownæŠ¥å‘Šã€‚
    
    Args:
        analysis_output (dict): åŒ…å«æŠ¥å‘Šå†…å®¹çš„å­—å…¸ã€‚
        root_dir (str): é¡¹ç›®çš„æ ¹ç›®å½• (conclusion/) çš„ç»å¯¹è·¯å¾„ã€‚
    """
    report_content = analysis_output.get("report_markdown")
    if not report_content:
        logger.error("åˆ†æç»“æœä¸­ç¼ºå°‘'report_content'ï¼Œæ— æ³•ä¿å­˜æŠ¥å‘Šã€‚")
        return

    # --- ä½¿ç”¨ root_dir æ„å»ºå¥å£®çš„æŠ¥å‘Šä¿å­˜è·¯å¾„ ---
    reports_dir = os.path.join(root_dir, 'analysis_reports')
    os.makedirs(reports_dir, exist_ok=True)
    
    timestamp_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    file_name = f"{timestamp_str}_analysis_result.md"
    file_path = os.path.join(reports_dir, file_name)

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        logger.info(f"åˆ†ææŠ¥å‘Šå·²æˆåŠŸä¿å­˜è‡³: {file_path}")
    except IOError as e:
        logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        raise

    # æ­¤å¤–ï¼Œä¹Ÿå°†ç»“æ„åŒ–æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­
    results_path = os.path.join(root_dir, 'data', 'results', 'analysis_results.json')
    try:
        all_results = []
        if os.path.exists(results_path):
            with open(results_path, 'r', encoding='utf-8') as f:
                try:
                    all_results = json.load(f)
                    if not isinstance(all_results, list):
                        all_results = []
                except json.JSONDecodeError:
                    all_results = []
        
        # åˆ›å»ºä¸€ä¸ªä»…åŒ…å«æ¨èç­–ç•¥çš„ç®€æ´æ¡ç›®
        structured_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "report_file": file_name,
            "diagnoses": analysis_output.get("diagnoses", []),
            "recommended_strategies": analysis_output.get("recommended_strategies", [])
        }
        all_results.append(structured_entry)

        # ä¿å­˜æ›´æ–°åçš„JSONæ•°æ®
        os.makedirs(os.path.dirname(results_path), exist_ok=True)
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æ„åŒ–åˆ†æç»“æœå¤±è´¥: {e}")